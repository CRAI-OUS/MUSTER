"""Multi Session Temporal Registration.

This module is the core of Multi Session Temporal Registration (MUSTER).

Classes:
    StageRegistration: Register a longitudinal series of images using a series of deformation fields using a single resolution.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
import scipy as sp

from alive_progress import alive_bar

from .core import field_calculus
from .core import losses
from .core import utils

import warnings


class BernsteinPolynomial(nn.Module):

    def __init__(self, degree, shape=None, coefficients=None):
        super(BernsteinPolynomial, self).__init__()
        self.degree = degree
        if coefficients is not None:
            self.coefficients = nn.Parameter(coefficients)
            self.shape = coefficients.shape[:-1]
        elif shape is not None:
            self.coefficients = nn.Parameter(torch.zeros(*shape, degree + 1))
            self.shape = shape
        else:
            raise ValueError("Either coefficients or shape must be provided.")
        print(f"Coefficent shape: {self.coefficients.shape}")
        print(f"Shape: {self.shape}")

    def forward(self, t):
        """
        Evaluate the Bernstein polynomial at timepoint t with given coefficients.
        
        Args:
        t (torch.Tensor): Tensor of shape (1,) containing a single timepoint.
        
        Returns:
        torch.Tensor: Tensor of shape (batch_size,) containing the evaluated polynomial values.
        """

        # Reshape the coefficients to (n_polynomials, degree + 1)
        coefficients = self.coefficients

        n_polynomials = coefficients.shape[0]
        degree = self.degree
        # Ensure that the number of coefficients matches the degree of the polynomial

        # Compute the Bernstein basis polynomials in parallel
        i = torch.arange(degree + 1, device=self.coefficients.device)

        basis = torch.stack([self.bernstein_basis(i, degree, t) for i in range(degree + 1)], dim=0)[None, None, None,
                                                                                                    None, None, :, 0]
        basis = basis.to(self.coefficients.device)

        # Compute the polynomial value using the basis and the coefficients
        result = torch.sum(self.coefficients * basis, dim=-1)

        # # Do average pooling over the result
        # result = torch.nn.functional.avg_pool3d(result, (3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))

        return result

    def bernstein_basis(self, i, n, t):
        """
        Compute the Bernstein basis polynomial of degree n at index i.
        
        Args:
        i (int): The index of the Bernstein basis polynomial.
        n (int): The degree of the polynomial.
        t (torch.Tensor): Tensor of shape (batch_size, 1) containing timepoints.
        
        Returns:
        torch.Tensor: Tensor of shape (batch_size, 1) containing the basis polynomial values.
        """
        binomial_coeff = torch.tensor(sp.special.comb(n, i), dtype=torch.float32)
        term = binomial_coeff * torch.pow(t, i) * torch.pow(1 - t, n - i)
        return term


class StageRegistration:
    """Register a longitudinal series of images using a series of deformation fields using a single resolution.

    The images must be in the same space and must be pre-registered using an rigid or affine transformation.

    The deformation between two consectutive timepoints is represented by a deformation field. These fields are 
    generated by integrating a vector field, also known as the deformation flow. Various methods for 
    this integration are supported:
        * ``integration_method=euler``: Use euler integration.
        * ``integration_method=rk4``: Use Runge-Kutta 4 integration.
       

    Supported image similarity metrics include:
        * ``img_similarity_metric="NCC"``: Normalized local cross correlation between the two images as loss. 
            The local neighborhood is a cube of size with side length ``img_similarity_spatial_size``.
        * ``img_similarity_metric="L2"``: L2 norm of the difference between the two images as loss. 
        * ``img_similarity_metric="NCCS"``: Use a sobel filter to compute the gradient of the two images, and use the 
            normalized local cross correlation between the two gradients as loss. The local window
            is a cube of size with side length ``3``.
        * ``img_similarity_metric="WNCC"``: Normalized local cross correlation between the two images as loss where each 
            local neighborhood is weighted by the cross standard deviation of the two images.
            The local neighborhood is a cube of size with side length ``img_similarity_spatial_size``.
        * ``img_similarity_metric="GaussNCC"``: Normalized local cross correlation between the two images as loss where
            each local neighborhood is weighted by a gaussian filter. The standard deviation of the gaussian filter is 
            ``img_similarity_spatial_size``.
        * ``img_similarity_metric="Fourier"``: Use the Fourier tranform to compute a filtered gradient of each images,
            and compute the global normalized cross correlation between the two filtered gradients as loss. 
            The standard deviation of the gaussian filter is ``img_similarity_spatial_size``.


    Args:
        image_size (tuple): The size of the input images in the format ``(x, y, z)``.
        pix_dim (tuple): The pixel dimensions of the input images in the format ``(dx, dy, dz)``. Default is ``(1, 1, 1)``.
        deform_res_scale (int): The resolution scale of the deformation field. The deformation field will have a 
            resolution of image_size/deform_res_scale. Default is ``1``.
        device (str): The device to use for computation. ('cpu' or 'cuda:n' or torch.device object). Default is ``"cpu"``.
        num_iterations (int): Number of optimization iterations. Default is ``100``.
        spatial_fo_smoothness_penalty (float): Weight for spatial smoothness in loss function of the first order derivative of the velocity field. Default is ``1.0``.
        spatial_so_smoothness_penalty (float): Weight for spatial smoothness in loss function of the second order derivative of the velocity field. Default is ``0.0``.
        l2_penalty (float): Weight for L2 penalty on the total length of the deformation field. Default is ``0.0``.
        smoothing_sigma (float): Sigma for Gaussian smoothing of deformation flows. Default is ``1.0``.
        mode (str): The mode for interpolation of the images. ('nearest', 'bilinear', or 'bicubic'). Default is ``"bilinear"``.
        integration_steps (int): The number of integration steps for integrating deformation flow. Default is ``7``.
        integration_method (str): The method for integrating deformation field.
            ('ss', 'euler', 'rk4', 'euler_time', or 'rk4'). Default is ``"ss"``.
        field_composition_method (str): The method for composing deformation fields.
            ('interpolate' or 'flow_continuation'). Default is ``"interpolate"``.
        affine_adjustment (str): The method for adjusting the affine transformation. ('none', 'rigid', or 'affine'). Default is ``"none"``.
        learning_rate (float): Learning rate for optimizer. Default is ``1e-3``.
        betas (tuple): The beta coefficients for Adam optimizer. Default is ``(0.9, 0.999)``.
        tol (float): Tolerance for optimization convergence. Default is ``1e-4``.
        img_similarity_metric (str): The image similarity metric ('NCC', 'L2', 'NCCS', 'WNCC', 'GaussNCC', or 'Fourier').
            Default is ``"NCC"``.
        img_similarity_spatial_size (int/float): Size or standard deviation of the local neighborhood for the image 
            similarity metric. Default is ``3``.
        verbose (bool): Flag for printing progress during optimization. Default is ``True``.
    """

    def __init__(
        self,
        image_size: tuple,
        pix_dim: tuple = (1, 1, 1),
        deform_res_scale: int = 1,
        device: str = "cpu",
        num_iterations: int = 100,
        spatial_fo_smoothness_penalty: float = 1.0,
        spatial_so_smoothness_penalty: float = 0.0,
        l2_penalty: float = 0.0,
        smoothing_sigma: float = 1.0,
        mode: str = "bilinear",
        integration_steps: int = 7,
        degree_polynomial: int = 2,
        integration_method: str = "euler",
        affine_adjustment: str = "none",
        learning_rate: float = 1e-3,
        betas: tuple = (0.9, 0.999),
        tol: float = 1e-4,
        img_similarity_metric: str = "VELLNGAUSS",
        img_similarity_spatial_size: int or float = 3,
        verbose: bool = True,
    ):
        # Valid values for integration_method
        valid_integration_methods = ["euler", "rk4"]
        if integration_method not in valid_integration_methods:
            raise ValueError(f"integration_method must be one of {valid_integration_methods}")

        # Valid values for img_similarity_metric
        valid_img_similarity_metrics = ["NCC", "L2", "NCCS", "Fourier", "WNCC", "GaussNCC", "VELLN", "VELLNGAUSS"]
        if img_similarity_metric not in valid_img_similarity_metrics:
            raise ValueError(f"img_similarity_metric must be one of {valid_img_similarity_metrics}")

        # Valid values for mode
        valid_modes = ["nearest", "bilinear", "bicubic"]
        if mode not in valid_modes:
            raise ValueError(f"mode must be one of {valid_modes}")

        # Check the dimensions of the image
        if len(image_size) != 3:
            raise ValueError("image_size must be a tuple of length 3")

        # Check the dimensions of the pixel dimensions
        if len(pix_dim) != 3:
            raise ValueError("pix_dim must be a tuple of length 3")

        self.device = torch.device(device)

        self.num_iterations = num_iterations
        self.spatial_fo_smoothness_penalty = spatial_fo_smoothness_penalty
        self.spatial_so_smoothness_penalty = spatial_so_smoothness_penalty
        self.l2_penalty = l2_penalty
        self.smoothing_sigma = smoothing_sigma
        self.integration_steps = integration_steps
        self.degree_polynomial = degree_polynomial
        self.integration_method = integration_method

        self.affine_adjustment = affine_adjustment
        self.betas = betas
        self.learning_rate = learning_rate
        self.image_size = image_size
        self.deform_res_scale = deform_res_scale
        self.verbose = verbose
        self.tol = tol
        self.img_similarity_metric = img_similarity_metric
        self.pix_dim = pix_dim
        self.img_similarity_spatial_size = img_similarity_spatial_size

        self.deform_size = [int(s // self.deform_res_scale) for s in self.image_size]
        self.deform_pix_dim = [s * self.deform_res_scale for s in self.pix_dim]
        self.mode = mode

        self.sp_tr = SpatialTransformer(
            self.deform_size,
            self.deform_pix_dim,
            mode=mode,
            padding_mode="border",
        ).to(self.device)

        if self.img_similarity_metric == "MSE" or self.img_similarity_metric == 'L2':
            self.img_sim_metric_fnc = losses.MSE()
        elif self.img_similarity_metric == "NCC":
            self.img_sim_metric_fnc = losses.NCC(self.img_similarity_spatial_size, scale_invariant=False)
        elif self.img_similarity_metric == "VELLN":
            self.img_sim_metric_fnc = losses.GroupVELLN(self.img_similarity_spatial_size, kernal_type='window')
        elif self.img_similarity_metric == "VELLNGAUSS":
            # self.img_sim_metric_fnc = losses.LogLinLoss(
            #     self.img_similarity_spatial_size,
            #     kernal_type='gaussian',
            #     pix_dim=self.pix_dim,
            #     image_size=self.image_size)
            # self.img_sim_metric_fnc = losses.NCC(self.img_similarity_spatial_size, scale_invariant=False)

            self.img_sim_metric_fnc = losses.GroupVELLN(
                self.img_similarity_spatial_size,
                kernal_type='gaussian',
                pix_dim=self.pix_dim,
                image_size=self.image_size)
            # self.img_sim_metric_fnc = losses.VELLN(
            #     kernal_size=self.img_similarity_spatial_size,
            #     kernal_type='gaussian',
            #     pix_dim=self.pix_dim,
            #     image_size=self.image_size)

        self.sp_tr_img = SpatialTransformer(self.image_size, pix_dim, mode=mode, padding_mode="zeros").to(self.device)

    def fit(
        self,
        images,
        inital_deform_flow=None,
        timepoints=None,
        initial_rotations=None,
        initial_translations=None,
        initial_affine=None,
        initial_bias_fields=None,
        masks=None,
    ):
        """Fits the deformation flow to the images.

        Args:
            images (torch.Tensor or np.ndarray): shape ``(N, channels, x, y, z)``
            inital_deform_flow (torch.Tensor or np.ndarray): shape ``(N-1, 3, x, y, z)``
            timepoints (torch.Tensor or np.ndarray): shape ``(N,)``
                The timepoints of the images. Used to adjust the temporal
                penalties. May be None, in which case the timepoints are
                assumed to be equally spaced.

        Returns:
            out (dict): Dictionary containing the deformation flow and optionally the rotations and translations.
        """

        self.n_img = images.shape[0]  # Number of images
        if self.integration_method in ['ss', 'euler', 'rk4']:
            self.n_flow = self.n_img - 1  # Number of deformations flow fields
        else:
            self.n_flow = self.n_img

        # Set device
        device = self.device

        images = self._convert_to_torch(images)

        if masks is None:
            masks = torch.ones_like(images)

        # The time difference between the images
        if timepoints is None:
            delta_times = torch.ones((self.n_img - 1, 1, 1, 1, 1), device=device)
        else:
            timepoints = self._convert_to_torch(timepoints)
            delta_times = timepoints[1:] - timepoints[:-1]
            delta_times = delta_times[:, None, None, None, None]

        dt = (timepoints[-1] - timepoints[0]) / self.integration_steps

        # The integral deformation flow
        if inital_deform_flow is None:
            #deform_flow = nn.parameter.Parameter(torch.zeros((self.n_flow, 3, *self.deform_size), device=device))
            deform_flow = BernsteinPolynomial(self.degree_polynomial, (1, 3, *self.deform_size)).to(device)
        else:
            inital_deform_flow = self._convert_to_torch(inital_deform_flow)
            print(inital_deform_flow.shape)
            deform_flow = BernsteinPolynomial(degree=self.degree_polynomial, coefficients=inital_deform_flow).to(device)

        param_groups = [{"params": [deform_flow.coefficients], "lr": self.learning_rate}]

        # Parameters for rigid ajustment of the images
        if self.affine_adjustment == "rigid":
            if initial_rotations is None:
                rotation_params = nn.Parameter(torch.zeros((self.n_img - 1, 3), device=device))
            else:
                initial_rotations = self._convert_to_torch(initial_rotations[1:])
                rotation_params = nn.Parameter(initial_rotations.clone().to(device))

            if initial_translations is None:
                translation_params = nn.Parameter(torch.zeros((self.n_img - 1, 3), device=device))
            else:
                initial_translations = self._convert_to_torch(initial_translations[1:])
                translation_params = nn.Parameter(initial_translations.clone().to(device))

            param_groups.append({
                "params": [rotation_params, translation_params],
                "lr": self.learning_rate * 0.03,
            })
        elif self.affine_adjustment == "affine":
            if initial_affine is None:
                affine_params = nn.Parameter((torch.eye(4, device=device)[:3, :]).repeat(self.n_img - 1, 1, 1))
            else:
                initial_affine = self._convert_to_torch(initial_affine)
                affine_params = nn.Parameter(initial_affine.clone().to(device))

            param_groups.append({
                "params": [affine_params],
                "lr": self.learning_rate * 0.001,
            })

        # Add a bias field similar to Ashburner et al. 2013
        if initial_bias_fields is None:
            bias_fields = nn.parameter.Parameter(torch.zeros((self.n_img, 2, *self.image_size), device=device))
        else:
            bias_fields = nn.parameter.Parameter(self._convert_to_torch(initial_bias_fields).clone().to(device))

        param_groups.append({"params": [bias_fields], "lr": self.learning_rate * 0.01})

        # if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
        #     log_sigmas = nn.parameter.Parameter(torch.ones((self.n_img, 1, 1, 1, 1), device=device) * (-1.0))
        #     param_groups.append({"params": [log_sigmas], "lr": self.learning_rate * 10})

        optimizer = torch.optim.Adam(param_groups, lr=self.learning_rate, betas=self.betas)
        scaler = torch.cuda.amp.GradScaler()

        # Cosine annealing with linear warmup
        warmup_steps = int(self.num_iterations * 0.2)
        scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1 / (warmup_steps) * (step + 1))
        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=int(self.num_iterations - warmup_steps))
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer, schedulers=[scheduler1, scheduler2], milestones=[warmup_steps])

        # Object for keeping track of convergence of the optimization process
        running_mean_var = utils.RunningMeanVar()

        use_autocast = True  #torch.cuda.is_available() and (str(device) != "cpu")

        with alive_bar(
                self.num_iterations,
                force_tty=True,
                max_cols=130,
                dual_line=True,
                title_length=50,
                elapsed="({elapsed})",
                stats="(eta: {eta})",
                disable=not self.verbose,
        ) as bar:
            for iteration_idx in range(self.num_iterations):
                optimizer.zero_grad()

                # Choose a random timepoint to be the center of the deformation field
                t_center = torch.rand(
                    (1,), device=device) * (timepoints[-1] - timepoints[0] - dt) + timepoints[0]  # TOFIX
                # print(f"first: {t_center}")
                # t_center = torch.tensor([0.01]).to(device)
                # print(f"second: {t_center}")
                # Choose a random image as the center of the deformation field
                # center_idx = torch.randint(0, self.n_img, (1,)).item()
                # center_idx = 0
                # t_center = timepoints[[center_idx]]

                # t_center = torch.tensor([0.5]).to(device)

                t_fwd = t_center
                t_bwd = t_center

                with torch.cuda.amp.autocast(use_autocast):
                    if self.affine_adjustment == "rigid":
                        # Add the identity transformation to first timepoint
                        rots = torch.cat((torch.zeros((1, 3), device=device), rotation_params), dim=0)
                        trans = torch.cat((torch.zeros((1, 3), device=device), translation_params), dim=0)
                        tran_grid = self._get_affine_grid(rots, trans)
                        # images_lin = self.sp_tr_img(images, tran_grid, displacement=False)
                        # masks_lin = self.sp_tr_img(masks, tran_grid, displacement=False)
                    elif self.affine_adjustment == "affine":  # TOFIX -- Must construct a true affine matrix
                        tran_grid = self._get_affine_grid(matrix=affine_params)
                        # Calculate the inverse of the affine matrix
                        affine_params_inv = torch.cat(
                            [affine_params,
                             torch.tensor([0, 0, 0, 1], device=device).repeat(self.n_img - 1, 1, 1)],
                            dim=1)
                        affine_params_inv = torch.inverse(affine_params_inv)[:, :3, :]

                        tran_grid = torch.cat((torch.zeros((1, 3, *self.image_size), device=device), tran_grid), dim=0)
                    else:
                        rots = torch.zeros((self.n_img, 3), device=device)
                        trans = torch.zeros((self.n_img, 3), device=device)
                        tran_grid = self._get_affine_grid(rots, trans)

                        # images_lin = images

                    # Do bias field correction
                    # bias_fields_centered = bias_fields  #- torch.mean(bias_fields, dim=(-1, -2, -3), keepdim=True)
                    # images_bias_corr = torch.exp((bias_fields_centered[:, [0]] + 1) * torch.log(images + 1e-6) +
                    #                              bias_fields_centered[:, [1]])
                    # images_bias_corr = (bias_fields_centered[:, [0]] + 1) * images + bias_fields_centered[:, [1]]

                    # images_bias_corr = images * torch.exp(bias_fields_centered)
                    images_bias_corr = images

                    # img_center = self.sp_tr_img(
                    #     images_bias_corr[[center_idx]], tran_grid[[center_idx]], displacement=False)
                    # mask_center = self.sp_tr_img(masks[[center_idx]], tran_grid[[center_idx]], displacement=False)

                    loss = 0
                    image_loss = 0

                    # The cumulative deformation fields
                    cum_deform_field_fwd = torch.zeros((1, 3, *self.image_size), device=device)
                    cum_deform_field_bwd = torch.zeros((1, 3, *self.image_size), device=device)

                    img_warped = torch.zeros_like(images_bias_corr)
                    masks_warped = torch.ones_like(masks)

                    # img_warped[center_idx] = img_center[0]
                    # masks_warped[center_idx] = mask_center[0]

                    fwd_end_reached = False
                    bwd_end_reached = False

                    total_length = 0

                    while not (fwd_end_reached and bwd_end_reached):
                        if not fwd_end_reached:
                            # Forward integration
                            # if the time to the next image is less than the dt, then use the remaining time as dt
                            comparison = ((timepoints - t_fwd) < dt) & ((timepoints - t_fwd) > 0)

                            if comparison.any():
                                dt_fwd = timepoints[torch.where(comparison)[0][0]] - t_fwd
                                img_reached_idx = torch.where(comparison)[0][0]
                            else:
                                dt_fwd = dt
                                img_reached_idx = None

                            # Check if end of timepoints is reached
                            if t_fwd + dt_fwd >= timepoints[-1]:
                                dt_fwd = timepoints[-1] - t_fwd
                                fwd_end_reached = True
                            cum_deform_field_fwd, length = self._intergate_flow_dt(
                                deform_flow, dir='fwd', dt=dt_fwd, t=t_fwd, cum_deform=cum_deform_field_fwd)
                            if img_reached_idx is not None:
                                img_warped[img_reached_idx] = self.sp_tr_img(
                                    images_bias_corr[[img_reached_idx]][None, ...],
                                    cum_deform_field_fwd + tran_grid[[img_reached_idx]],
                                    displacement=False)[0]
                                masks_warped[img_reached_idx] = self.sp_tr_img(
                                    masks[[img_reached_idx]][None, ...],
                                    cum_deform_field_fwd + tran_grid[[img_reached_idx]],
                                    displacement=False)[0]

                            total_length += length
                            t_fwd = t_fwd + dt_fwd

                        if not bwd_end_reached:
                            # Backward integration
                            # if the time to the next image is less than the dt, then use the remaining time as dt
                            comparison = ((t_bwd - timepoints) < dt) & ((t_bwd - timepoints) > 0)
                            if comparison.any():
                                dt_bwd = t_bwd - timepoints[torch.where(comparison)[0][0]]
                                img_reached_idx = torch.where(comparison)[0][0]
                            else:
                                dt_bwd = dt
                                img_reached_idx = None
                            # Check if end of timepoints is reached
                            if t_bwd - dt_bwd <= timepoints[0]:
                                dt_bwd = t_bwd - timepoints[0]
                                bwd_end_reached = True
                            cum_deform_field_bwd, length = self._intergate_flow_dt(
                                deform_flow, dir='bwd', dt=dt_bwd, t=t_bwd, cum_deform=cum_deform_field_bwd)
                            if img_reached_idx is not None:
                                img_warped[img_reached_idx] = self.sp_tr_img(
                                    images_bias_corr[[img_reached_idx]][None, ...],
                                    cum_deform_field_bwd + tran_grid[[img_reached_idx]],
                                    displacement=False)[0]
                                masks_warped[img_reached_idx] = self.sp_tr_img(
                                    masks[[img_reached_idx]][None, ...],
                                    cum_deform_field_bwd + tran_grid[[img_reached_idx]],
                                    displacement=False)[0]

                            total_length += length
                            t_bwd = t_bwd - dt_bwd

                    image_loss, I = self.img_sim_metric_fnc.loss(images=img_warped, masks=masks_warped)
                    # I = img_warped[[0]]
                    # image_loss = self.img_sim_metric_fnc.loss(I, img_warped[[1]])

                    # I = torch.mean(img_warped * masks_warped, dim=0) / torch.mean(masks_warped, dim=0)

                    # masks_warped_prod = torch.prod(masks_warped, dim=0, keepdim=True)
                    # image_loss = torch.mean((I - img_warped)**2 * masks_warped_prod) / torch.mean(masks_warped_prod)

                    # y_true, y_pred, sigma_true, sigma_pred, mask=None):
                    # image_loss = self.img_sim_metric_fnc.loss(
                    #     y_true=img_center.repeat(self.n_img, 1, 1, 1, 1),
                    #     y_pred=img_warped,
                    #     mask=None,
                    #     sigma_true=torch.tensor([0.01], device=device),
                    #     sigma_pred=torch.tensor([0.01], device=device))

                    # image_loss = torch.mean((img_center - img_warped)**2)

                    deform_flow_coefs = deform_flow.coefficients[0].permute(4, 0, 1, 2, 3)
                    loss += self._spatial_continuity_loss(deform_flow_coefs)
                    # loss += self._spatial_continuity_loss_bias(bias_fields)
                    # loss += torch.mean(bias_fields**2) * 0.1
                    loss += self.l2_penalty * total_length
                    # Combine the losses
                    loss += image_loss

                if use_autocast:
                    # Propagate the loss and update the parameters
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    optimizer.step()

                # Ignore the warning from the scheduler
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    scheduler.step()

                bar()
                bar.title(f"\tLoss: {loss.item():.4e}, Image Loss: {image_loss.item():.4e}")
                bar.title_length = 200

                running_mean_var.add(loss.item())

                if self.verbose:
                    if iteration_idx == self.num_iterations - 1:
                        bar.title(f"\tMax iterations reached")

                # if (iteration_idx > 20 and np.sqrt(running_mean_var.var()) / running_mean_var.mean() < self.tol):
                #     bar.title(f"\tConvergence reached")
                #     break

        optimizer.zero_grad()

        with torch.no_grad():
            images_lin = self.sp_tr_img(images, tran_grid, displacement=False)

            return_dict = {
                "bias_field": bias_fields.detach().cpu().numpy(),
                "deform_field": self._get_cum_flow_matrix(deform_flow, timepoints),
                "coefficients": deform_flow.coefficients,
                "affine_adjusted_images": images_lin.detach().cpu().numpy(),
                "I": I.detach().cpu().numpy(),
            }
            if self.affine_adjustment == "rigid":
                return_dict["rotations"] = rots.detach().cpu().numpy()
                return_dict["translations"] = trans.detach().cpu().numpy()
            elif self.affine_adjustment == "affine":
                return_dict["affine_matrix"] = affine_params.detach().cpu().numpy()

        return return_dict

    def _get_homogeneous_transformation_matrix(self, rotation, translation):
        """Computes the affine matrix from rotation and translation
        """
        Rx = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Rx[:, [1, 2], [1, 2]] = torch.cos(rotation[:, 0])[:, None]
        Rx[:, 1, 2] = -torch.sin(rotation[:, 0])
        Rx[:, 2, 1] = torch.sin(rotation[:, 0])

        Ry = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Ry[:, [0, 2], [0, 2]] = torch.cos(rotation[:, 1])[:, None]
        Ry[:, 0, 2] = torch.sin(rotation[:, 1])
        Ry[:, 2, 0] = -torch.sin(rotation[:, 1])

        Rz = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Rz[:, [0, 1], [0, 1]] = torch.cos(rotation[:, 2])[:, None]
        Rz[:, 0, 1] = -torch.sin(rotation[:, 2])
        Rz[:, 1, 0] = torch.sin(rotation[:, 2])

        R = torch.matmul(Rz, torch.matmul(Ry, Rx))
        matrix = torch.cat([R, translation[:, :, None]], dim=2)
        return matrix

    def _get_affine_grid(self, rotation=None, translation=None, matrix=None, type='img'):
        """
        Computes the affine grid from rotation and translation
        """
        if matrix is None:
            matrix = self._get_homogeneous_transformation_matrix(rotation, translation)

        if type == 'img':
            grid = self.sp_tr_img.grid
        else:
            grid = self.sp_tr.grid

        # Grid is in the shape (N, 3, H, W, D)
        # Matrix is in the shape (N, 3, 4)
        grid = (torch.einsum("nij, njhwd -> nihwd", matrix[:, :3, :3], grid) + matrix[:, :3, 3, None, None, None])
        return grid

    def _spatial_continuity_loss(self, deform_flow):

        J = field_calculus.jacobian(deform_flow, pix_dim=self.deform_pix_dim)
        margin = 0
        loss = 0
        # Normal distribution prior over the Jacobian
        loss += torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**
                           2) * self.spatial_fo_smoothness_penalty
        if self.spatial_so_smoothness_penalty > 0:
            L = field_calculus.laplacian(J, pix_dim=self.deform_pix_dim)
            loss += torch.mean(L[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**
                               2) * self.spatial_so_smoothness_penalty
        return loss

    def _spatial_continuity_loss_bias(self, bias_field):

        J = field_calculus.jacobian(bias_field, pix_dim=self.deform_pix_dim)
        margin = 0
        loss = 0
        # Normal distribution prior over the Jacobian
        loss += torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**2) * 100000
        # L = field_calculus.laplacian(J, pix_dim=self.deform_pix_dim)
        # loss += torch.mean(L[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**2) * 100
        return loss

    def _get_cum_flow_matrix(self, deform_flow, timepoints):
        """
        Computes the cumulative flow matrix for the given timepoints.
        """
        cum_flow = torch.zeros((len(timepoints), len(timepoints), 3, *self.image_size), device=self.device)
        dt = torch.abs(timepoints[-1] - timepoints[0]) / self.integration_steps

        for image_idx, start_time in enumerate(timepoints):
            if image_idx == 0:
                # Integrate only in forward direction
                cum_flow[image_idx, :] = self._integrate_flow(timepoints[:], deform_flow, dt, direction='fwd')
            elif image_idx == len(timepoints) - 1:
                # Integrate only in backward direction
                cum_flow[image_idx, :] = self._integrate_flow(timepoints[:], deform_flow, dt, direction='bwd')
            else:
                cum_flow[image_idx, :image_idx + 1] = self._integrate_flow(
                    timepoints[:image_idx + 1], deform_flow, dt, direction='bwd')
                cum_flow[image_idx, image_idx:] = self._integrate_flow(
                    timepoints[image_idx:], deform_flow, dt, direction='fwd')
        return cum_flow

    def _integrate_flow(self, timepoints, deform_flow, dt, direction='fwd'):
        """
        Integrates a vector field via scaling and squaring.
        Args:
            deform_flow: The vector field to integrate. Shape (N, 3, x, y, z)
            dir: The direction to integrate in. ('fwd' or 'bwd')
            target_grid: The grid to start the integration from. If None, the grid will be set to the identity transformation.
        """
        cum_flow = torch.zeros((len(timepoints), 3, *self.image_size), device=self.device)

        if direction == 'fwd':
            for image_idx, start_time in enumerate(timepoints):
                # Integrate only in forward direction
                cum_deform_field_fwd = torch.zeros((1, 3, *self.image_size), device=self.device)
                time = torch.tensor([start_time], device=self.device)
                next_idx = image_idx + 1

                while next_idx < len(timepoints):
                    if time + dt >= timepoints[next_idx]:
                        dt_step = torch.abs(timepoints[next_idx] - time)
                        img_reached = True
                    else:
                        dt_step = dt
                        img_reached = False
                    cum_deform_field_fwd, _ = self._intergate_flow_dt(
                        deform_flow, dir='fwd', dt=dt_step, t=time, cum_deform=cum_deform_field_fwd)
                    time = time + dt_step
                    if img_reached:
                        cum_flow[next_idx] = cum_deform_field_fwd
                        next_idx += 1
        else:
            for image_idx, start_time in enumerate(timepoints):
                # Integrate only in backward direction
                cum_deform_field_bwd = torch.zeros((1, 3, *self.image_size), device=self.device)
                time = torch.tensor([start_time], device=self.device)
                next_idx = image_idx - 1

                while next_idx >= 0:
                    if time - dt <= timepoints[next_idx]:
                        dt_step = torch.abs(time - timepoints[next_idx])
                        img_reached = True
                    else:
                        dt_step = dt
                        img_reached = False
                    cum_deform_field_bwd, _ = self._intergate_flow_dt(
                        deform_flow, dir='bwd', dt=dt_step, t=time, cum_deform=cum_deform_field_bwd)
                    time = time - dt_step
                    if img_reached:
                        cum_flow[next_idx] = cum_deform_field_bwd
                        next_idx -= 1
        return cum_flow

    def _intergate_flow_dt(self, deform_flow, dir, t, dt, cum_deform):
        """
        Integrates a vector field via scaling and squaring.
        Args:
            deform_flow: The vector field to integrate. Shape (N, 3, x, y, z)
            dir: The direction to integrate in. ('fwd' or 'bwd')
            target_grid: The grid to start the integration from. If None, the grid will be set to the identity transformation.
        """
        super().__init__()
        nsteps = self.integration_steps
        method = self.integration_method

        # if method == "euler":

        # Integrate a constant vector field with euler integration
        if dir in ["backward", "bwd"]:
            dt = -dt

        if method == "euler":
            if self.smoothing_sigma > 0:
                flow = self._gaussian_smooth(deform_flow(t), self.smoothing_sigma)
            else:
                flow = deform_flow(t)
            deform_flow_t = self.sp_tr_img(flow * dt, cum_deform, displacement=True)
            cum_deform = cum_deform + deform_flow_t
        elif method == "rk4":
            k1 = self.sp_tr_img(deform_flow(t) * dt, cum_deform, displacement=True)
            deform_flow_inter = deform_flow(t + 0.5 * dt) * dt
            k2 = self.sp_tr_img(deform_flow_inter, cum_deform + k1 * 0.5, displacement=True)
            k3 = self.sp_tr_img(deform_flow_inter, cum_deform + k2 * 0.5, displacement=True)
            k4 = self.sp_tr_img(deform_flow(t + dt) * dt, cum_deform + k3, displacement=True)
            deform_flow_t = (k1 + 2 * k2 + 2 * k3 + k4) / 6.0
            cum_deform = cum_deform + deform_flow_t
        return cum_deform, torch.mean(torch.sqrt(torch.sum(deform_flow_t**2 + 0.0001, dim=1)))

    def _gaussian_smooth(self, field, sigma):
        """Smooths the deformation field with a Gaussian filter in the Fourier domain.

        Args:
            field (torch.Tensor): The deformation field to be smoothed.
            sigma (float): The standard deviation of the Gaussian filter.

        Returns:
            torch.Tensor: The smoothed deformation field.
        """
        v_fft = torch.fft.fftn(field, dim=[2, 3, 4])

        freqx = torch.fft.fftfreq(field.shape[2], d=self.deform_pix_dim[0]).to(self.device)
        freqy = torch.fft.fftfreq(field.shape[3], d=self.deform_pix_dim[1]).to(self.device)
        freqz = torch.fft.fftfreq(field.shape[4], d=self.deform_pix_dim[2]).to(self.device)

        freqx, freqy, freqz = torch.meshgrid(freqx, freqy, freqz, indexing="ij")
        omega_f = 1 / (2 * torch.pi * sigma)
        filter_response = torch.exp(-1 / 2 * ((freqx**2 + freqy**2 + freqz**2) / omega_f**2))
        v_fft = filter_response * v_fft

        field_filtered = torch.real(torch.fft.ifftn(v_fft, dim=[2, 3, 4]))
        return field_filtered

    def deform(self, images, deform_field, mode="bilinear", padding_mode="zeros", displacement=True):
        """Transforms the batch of images according to the deformation field.
        
        Args:
            images (torch.Tensor or np.ndarray): The images to deform. Shape ``(N, C, x, y, z)``
            deform_field (torch.Tensor or np.ndarray): The deformation field. Shape ``(N, 3, x, y, z)``
            mode (str, optional): The interpolation mode. Either 'bilinear' or 'nearest'.
            padding_mode (str, optional): The padding mode. Either 'zeros' or 'border'.
            displacement (bool, optional): Whether the deformation field is a displacement field or a deformation field.
            
        Returns:
            torch.Tensor or np.ndarray: The deformed images. Shape ``(N, C, x, y, z)``
        """
        if isinstance(images, np.ndarray):
            dtype = "np"
        elif isinstance(images, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("images must be either numpy array or torch tensor")

        images = self._convert_to_torch(images)
        deform_field = self._convert_to_torch(deform_field)

        if ((mode == self.mode) and np.all(np.array(self.image_size) == np.array(images.shape[2:]))):
            with torch.no_grad():
                def_images = self.sp_tr_img(images, deform_field)
        else:
            sp_tr = SpatialTransformer(
                images.shape[2:],
                self.pix_dim,
                mode=mode,
                padding_mode=padding_mode,
            ).to(self.device)
            def_images = sp_tr(images, deform_field, displacement=displacement)

        if dtype == "np":
            return def_images.detach().cpu().numpy()
        elif dtype == "torch":
            return def_images

    def rigid_transform(self, images, rotations, translations):
        grid = self._get_affine_grid(rotations, translations)
        return self.sp_tr_img(images, grid, displacement=False)

    def linear_transform(self, images, matrix):
        grid = self._get_affine_grid(matrix=matrix)
        return self.sp_tr_img(images, grid, displacement=False)

    def add_linear_transform(self, displacements, matrix):
        if isinstance(displacements, np.ndarray):
            dtype = "np"
        elif isinstance(displacements, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("displacements must be either numpy array or torch tensor")

        displacements = self._convert_to_torch(displacements)
        matrix = self._convert_to_torch(matrix)

        deform_field = displacements + self._get_affine_grid(matrix=matrix)
        # deform_field = torch.cat([deform_field, torch.ones_like(deform_field[:, :1, ...])], dim=1)
        # deform_field = torch.einsum("nji, nihwd -> njhwd", matrix, deform_field)

        deform_field = deform_field[:, :3, ...] - self.sp_tr_img.grid
        if dtype == "np":
            return deform_field.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_field

    def _convert_to_torch(self, data):
        """
        Converts the data to torch tensor.
        """
        if isinstance(data, np.ndarray):
            return torch.from_numpy(data).to(dtype=torch.float32, device=self.device)
        elif isinstance(data, torch.Tensor):
            return data.to(dtype=torch.float32, device=self.device)
        elif isinstance(data, list):
            return torch.tensor(data, dtype=torch.float32, device=self.device)
        else:
            raise TypeError("Data must be either list, numpy array or torch tensor")


class Registration:
    """Multi Stage Temporal Registration.
    Registrates a series of images. The images are first registrated at a low resolution and then the
    resolution is increased and the images are registrated again. At each resolution the deformation field is
    has a resolution relative to the resampled image resolution set by ``deform_res_scale``.
    
    Accepts same arguments as :class:`StageRegistration` but :attr:`iterations` is replaced by 
    :attr:`stages_iterations`, attr:`deform_res_scale` is replaced by :attr:`stages_deform_scales`, and
    :attr:`image_size` is replaced by :attr:`stages_img_scales`.
    
    Example:
        >>> from muster import Registration
        >>> deform_reg = Registration(
        >>>     stages_iterations=[500, 250, 100],
        >>>     stages_img_scales=[4, 2, 1],
        >>>     stages_deform_scales=[4, 2, 2],
        >>>     image_size=[128, 128, 128],
        >>>     pix_dim=[1, 1, 1],
        >>>     device="cuda:0",
        >>> )
        >>> out = deform_reg.fit(images)
    This example will registrate the image first at a resolution of ``[32, 32, 32]`` with a deformation grid of 
    ``[8, 8, 8]``. Then the resolution is increased to ``[64, 64, 64]`` with a deformation grid of ``[32, 32, 32]``.
    Finally the resolution is increased to ``[128, 128, 128]`` with a deformation grid of ``[64, 64, 64]``.
    
    See :class:`StageRegistration` for more information about the arguments.
    
    Args:
        stages_iterations (list of int): Number of iterations for each stage.
        stages_img_scales (list of int): Image rescaling factors for each stage.
        stages_deform_scales (list of int): Deformation rescaling factors for each stage relative to the image rescaling factor.
        """

    def __init__(self, stages_iterations: list, stages_img_scales: list, stages_deform_scales: list, **kwargs):

        # Make sure the arguments are the same length
        if len(stages_iterations) != len(stages_img_scales) or len(stages_img_scales) != len(stages_deform_scales):
            raise ValueError("stages_iterations, stages_img_scales and stages_deform_scales must have the same length")

        self.stages_iterations = stages_iterations
        self.device = kwargs.get("device", "cpu")
        self.image_sizes = []
        self.deform_scales = stages_deform_scales
        self.image_size = kwargs["image_size"]
        self.pix_dim = kwargs["pix_dim"]
        self.deform_sizes = []
        self.pix_dims = []
        self.deform_pix_dims = []
        self.verbose = kwargs.get("verbose", False)
        self.affine_adjustment = kwargs.get("affine_adjustment", "none")

        for stage in range(len(stages_iterations)):
            img_size = np.array(kwargs["image_size"]) // stages_img_scales[stage]
            self.image_sizes.append(img_size.tolist())
            self.pix_dims.append(np.array(kwargs["pix_dim"]) * stages_img_scales[stage])

            self.deform_sizes.append(
                ((np.array(self.image_sizes[stage]) // self.deform_scales[stage]).astype(np.int32)).tolist())
            self.deform_pix_dims.append(np.array(self.pix_dims[stage]) * self.deform_scales[stage])

        # Initialize the deformable registration model of the last stage
        self.kwargs = kwargs
        stage_args = self.kwargs
        stage_args["deform_res_scale"] = self.deform_scales[-1]
        stage_args["num_iterations"] = self.stages_iterations[-1]
        stage_args["image_size"] = self.image_sizes[-1]
        stage_args["pix_dim"] = self.pix_dims[-1]

        self.dr = StageRegistration(**stage_args)

    def fit(self, images, timepoints=None, masks=None):
        """
        Fits the registration model to the images
        Args:.
            - images (numpy.ndarray or torch.Tensor): The images to registrate. Shape (N, C, x, y, z)
            - timepoints (numpy.ndarray or torch.Tensor, optional): The timepoints of the images. Shape (N,)
            - masks (numpy.ndarray or torch.Tensor, optional): The masks of the images. Shape (N, C, x, y, z)

        Returns:
            dict: A dictionary containing the deformation field, rotations and translations.
        
        """

        images = self._convert_to_torch(images)

        # Register the images to the current stage resolution
        deform_flow = None
        rotation = None
        translation = None
        affine_matrix = None
        bias_fields = None
        for stage in range(len(self.stages_iterations)):

            # Reinitalize the deformable registration model for the current stage
            stage_args = self.kwargs
            stage_args["deform_res_scale"] = self.deform_scales[stage]
            stage_args["num_iterations"] = self.stages_iterations[stage]
            stage_args["image_size"] = self.image_sizes[stage]
            stage_args["pix_dim"] = self.pix_dims[stage]

            self.dr = None
            torch.cuda.empty_cache()
            self.dr = StageRegistration(**stage_args)

            if self.verbose:
                print(
                    f"Stage {stage+1}/{len(self.stages_iterations)} at image resolution {self.image_sizes[stage]}, deformation resolution {self.deform_sizes[stage]}"
                )
            # Resample the images to the current stage resolution
            img_resampled = nn.functional.interpolate(
                images,
                size=self.image_sizes[stage],
                mode="trilinear",
                align_corners=True,
            )
            if masks is not None:
                masks_resampled = nn.functional.interpolate(masks, size=self.image_sizes[stage], mode="trilinear")
            else:
                masks_resampled = None

            if bias_fields is not None:
                bias_fields = nn.functional.interpolate(bias_fields, size=self.image_sizes[stage], mode="trilinear")
            else:
                bias_fields = None

            out = self.dr.fit(
                images=img_resampled,
                inital_deform_flow=deform_flow,
                timepoints=timepoints,
                initial_rotations=rotation,
                initial_translations=translation,
                initial_affine=affine_matrix,
                initial_bias_fields=bias_fields,
                masks=masks_resampled,
            )

            deform_flow = out["coefficients"][0]
            if self.affine_adjustment == "rigid":
                rotation = torch.tensor(out["rotations"], device=self.device)
                translation = torch.tensor(out["translations"], device=self.device)
            elif self.affine_adjustment == "affine":
                affine_matrix = torch.tensor(out["affine_matrix"], device=self.device)

            # out["deform_flow_not_resampled"] = (deform_flow.detach().cpu().numpy())
            print(f"Deform flow shape: {deform_flow.shape}")
            deform_flow = deform_flow.permute(4, 0, 1, 2, 3)
            if stage < len(self.stages_iterations) - 1:
                deform_flow = nn.functional.interpolate(
                    deform_flow,
                    size=self.deform_sizes[stage + 1],
                    mode="trilinear",
                    align_corners=True,
                ).permute(1, 2, 3, 4, 0)[None, ...]
        # Add the deform fields
        #out["deform_field"] = self.get_deform_fields(deform_flow).detach().cpu().numpy()
        # out["rotations"] = rotation.detach().cpu().numpy()
        # out["translations"] = translation.detach().cpu().numpy()
        #out["affine_matrix"] = affine_matrix.detach().cpu().numpy()
        return out

    def rigid_transform(self, images, rotations, translations):
        """
        Applies the rigid transformation to the images.
        """
        rotations = self._convert_to_torch(rotations)
        translations = self._convert_to_torch(translations)
        return self.dr.rigid_transform(images, rotations, translations)

    def deform(self, images, deform_field, mode="bilinear", padding_mode="zeros", displacement=True):
        """Transforms the batch of images according to the deformation field.
        
        Args:
            images (torch.Tensor or np.ndarray): The images to deform. Shape ``(N, C, x, y, z)``
            deform_field (torch.Tensor or np.ndarray): The deformation field. Shape ``(N, 3, x, y, z)``
            mode (str, optional): The interpolation mode. Either 'bilinear' or 'nearest'.
            padding_mode (str, optional): The padding mode. Either 'zeros' or 'border'.
            displacement (bool, optional): Whether the deformation field is a displacement field or a deformation field.
            
        Returns:
            torch.Tensor or np.ndarray: The deformed images. Shape ``(N, C, x, y, z)``
        """
        return self.dr.deform(images, deform_field, mode, padding_mode, displacement)

    def get_deform_fields(self, deform_flow):
        """Computes the all deformation fields from the deformation flow.
        
        The deformations are organized in a matrix such that the deformation from the deformation that distortes 
        image_i to image_j is given by deform_matrix[j, i]. Another way of viewing the matrix is that the 
        the displacment at time t_j of a particle starting at the identity grid in image_i is given with
        deform_matrix[i, j]. 
        
        Args:
            deform_flow (torch.Tensor or np.ndarray): The deformation flow. Shape ``(N, 3, x, y, z)``
            
        Returns:
            torch.Tensor or np.ndarray: The deformation fields. Shape ``(N, N, 3, x, y, z)``
        """

        if isinstance(deform_flow, np.ndarray):
            dtype = "np"
        elif isinstance(deform_flow, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("deform_field must be either numpy array or torch tensor")

        deform_flow = self._convert_to_torch(deform_flow)
        if self.deform_scales[-1] != 1:
            with torch.no_grad():
                deform_matrix = self.dr.get_deform_fields(deform_flow)
                n_img = deform_matrix.shape[0]
                deform_matrix = nn.functional.interpolate(
                    deform_matrix.reshape((-1, 3, *self.deform_sizes[-1])),
                    size=self.image_size,
                    mode="trilinear",
                    align_corners=True,
                )
                deform_matrix = deform_matrix.reshape((
                    n_img,
                    n_img,
                    3,
                    *self.image_size,
                ))
        else:
            with torch.no_grad():
                deform_matrix = self.dr.get_deform_fields(deform_flow)

        if dtype == "np":
            return deform_matrix.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_matrix

    def _convert_to_torch(self, data):
        """Converts the data to torch tensor.
        
        Args:
            data (numpy.ndarray or torch.Tensor): The data to convert.
            
        Returns:
            torch.Tensor: The data converted to torch tensor.
        """
        if isinstance(data, np.ndarray):
            return torch.from_numpy(data).to(dtype=torch.float32, device=self.device)
        elif isinstance(data, torch.Tensor):
            return data.to(dtype=torch.float32, device=self.device)
        else:
            raise TypeError("Data must be either numpy array or torch tensor")

    def get_identity_grid(self):
        return self.dr.sp_tr_img.grid.detach().cpu().numpy()


class SpatialTransformer(nn.Module):
    r""" Spatial Transformer for performing grid pull operations on spaces.
    Based on https://github.com/voxelmorph/voxelmorph. The spatial transformer uses a deformation field to deform
    the input space. The deformation field is given in mm/step. 
    
    Args:
        size (tuple): the target size of the output tensor. shape ``(x, y, z)``
        pix_dim (tuple): the pixel spacing (mm/px) of the output tensor ``(dx, dy, dz)``
        mode ('bilinear' or 'nearest'): interpolation mode
        padding_mode ('zeros' or 'border'): padding mode
    """

    def __init__(self, size, pix_dim=(1, 1, 1), mode="bilinear", padding_mode="zeros"):
        super().__init__()

        self.mode = mode
        self.padding_mode = padding_mode
        self.pix_dim = pix_dim
        self.size = size
        self.size_mm = [(s - 1) * p for s, p in zip(size, pix_dim)]

        # Create sampling grid
        vectors = [torch.linspace(0, self.size_mm[i], steps=self.size[i]) for i in range(len(self.size))]
        grids = torch.meshgrid(vectors, indexing='ij')
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.float)
        self.register_buffer("grid", grid)

    def forward(self, field, transformation_field, displacement=True):
        r"""Deforms the field image with the transformation field with units in mm/step.

            Args:
                field: the source space to be transformed
                transformation_field: the transformation field to be applied to the source image, with units in mm/step
                displacement: whether the transformation field is a displacement field or a deformation field
                
            Returns:
                torch.Tensor: the transformed source image
            """

        if displacement:
            new_locs = self.grid + transformation_field
        else:
            new_locs = transformation_field.clone()

        # need to normalize grid values to [-1, 1] for resampler
        for axis_idx in range(len(self.size_mm)):
            new_locs[:, axis_idx, ...] = 2 * (new_locs[:, axis_idx, ...] / (self.size_mm[axis_idx]) - 0.5)

        if len(self.size_mm) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(self.size_mm) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(
            field,
            new_locs,
            align_corners=True,
            mode=self.mode,
            padding_mode=self.padding_mode,
        )


class AffineRegistration:

    def __init__(self,
                 n_search_angles,
                 stages_img_scales,
                 image_size,
                 pix_dim,
                 mode="bilinear",
                 padding_mode="zeros",
                 device="cpu"):
        self.n_search_angles = n_search_angles
        self.stages_img_scales = stages_img_scales

        self.image_size = (torch.tensor(image_size) + stages_img_scales[0] - 1) // stages_img_scales[0]
        print(f"Image size: {self.image_size}")
        self.pix_dim = np.array(pix_dim) * stages_img_scales[0]
        self.mode = mode
        self.padding_mode = padding_mode
        self.device = device
        self.sp_tr_img = SpatialTransformer(
            size=self.image_size,
            pix_dim=self.pix_dim,
            mode=self.mode,
            padding_mode=self.padding_mode,
        ).to(self.device)

    def fit(self, fixed, movings):
        """ Fits the affine registration model to the images.
        
        Args:
            fixed (torch.Tensor): The fixed image. Shape (1, C, x, y, z)
            movings (torch.Tensor): The moving images. Shape (N, C, x, y, z)
        """

        # Z normalize the images
        fixed /= fixed.std()
        movings /= movings.std(axis=(1, 2, 3, 4), keepdims=True)

        # Downsample the images
        fixed = torch.nn.functional.interpolate(
            fixed, size=tuple(self.image_size), align_corners=True, mode='trilinear')
        movings = torch.nn.functional.interpolate(
            movings, size=tuple(self.image_size), align_corners=True, mode='trilinear')

        with torch.no_grad():
            # Calculate the center of mass of the fixed image
            fixed_cm = self._calculate_cm(fixed)

            # Calculate the center of mass of the moving images
            movings_cm = self._calculate_cm(movings)

            # Initalize the affine matrix
            affine_mats = torch.zeros((movings.shape[0], 3, 4), device=fixed.device)

            # Set the translation
            affine_mats[:, :3, 3] = fixed_cm - movings_cm

            # Calculate the moments of the fixed image
            fixed_moments = self._calculate_moments(fixed, fixed_cm)

            # Calculate the moments of the moving images
            movings_moments = self._calculate_moments(movings, movings_cm).to(fixed.device)

            scale_mat = torch.zeros((movings.shape[0], 3, 3), device=fixed.device)
            # Set the scaling
            for i in range(3):
                affine_mats[:, i, i] = 1
                scale_mat[:, i, i] = movings_moments[:, i] / fixed_moments[:, i]

            rot_mats = self._get_uniform_rotations(self.n_search_angles**3)

            # Repeat the affine matrices for each rotation
            affine_mats = affine_mats.repeat_interleave(self.n_search_angles**3, dim=0)
            movings_cm = movings_cm.repeat_interleave(self.n_search_angles**3, dim=0)

            # Rotate the affine matrices around the center of mass
            affine_mats = self._rotate_affine_mats(affine_mats, rot_mats, movings_cm)
            affine_mats = self._rotate_affine_mats(affine_mats, scale_mat, movings_cm)

        epochs = 200
        fixed = fixed.repeat_interleave(self.n_search_angles**3 * movings.shape[0], dim=0).detach().clone()
        movings = movings.repeat_interleave(self.n_search_angles**3, dim=0).detach().clone()
        print(affine_mats)
        affine_mats = torch.nn.Parameter(affine_mats, requires_grad=True)
        #optimizer = torch.optim.Adam([affine_mats], lr=0.001)
        #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)

        options = {'lr': 0.01, 'gtol': 1e-24, 'xtol': 1e-24}
        #optimizer = Minimizer([affine_mats], method='bfgs', max_iter=epochs, disp=2, options=options)

        # #loss_fn = losses.GaussNCC(13, image_size=self.image_size, pix_dim=self.pix_dim, reduce=False)
        loss_fn = losses.MutualInformation(sigma=0.01, num_bins=32, normalize=True, reduce=False).to(self.device)

        #loss_fn = losses.WNCC(3, reduce=False)

        #loss_fn = losses.NCC(3, reduce=False)

        # for epoch in range(epochs):
        #     optimizer.zero_grad()

        #     warped_movings = self._affine_transform(movings, affine_mats, cm=movings_cm)

        #     #loss = torch.mean((fixed - warped_movings)**2, dim=(1, 2, 3, 4))

        #     loss = loss_fn.loss(fixed, warped_movings)
        #     best_loss, best_idx = torch.min(loss, dim=0)

        #     loss_mean = loss.mean()
        #     loss_mean.backward()
        #     optimizer.step()
        #     scheduler.step()

        #     print(f"Epoch {epoch+1}/{epochs}, loss: {best_loss.item()}, idx: {best_idx.item()}")

        def closure():
            optimizer.zero_grad()
            warped_movings = self._affine_transform(movings, affine_mats, cm=movings_cm)
            loss = loss_fn.loss(fixed, warped_movings)
            #loss = torch.mean((fixed - warped_movings)**2, dim=(1, 2, 3, 4))
            loss_mean = loss.mean()
            # Calculate the determinant of the affine matrix
            det = torch.det(affine_mats[:, :3, :3])
            #loss_mean += 0.02 * torch.mean((det - 1)**2)

            return loss_mean

        loss = optimizer.step(closure)

        # Compute one last time to get the best affine matrix
        warped_movings = self._affine_transform(movings, affine_mats, cm=movings_cm)
        loss = loss_fn.loss(fixed, warped_movings)

        # Return the best affine matrix
        best_loss, best_idx = torch.min(loss, dim=0)
        print(f"Best loss: {best_loss.item()}, idx: {best_idx.item()}")
        return affine_mats[best_idx][None, ...]

        # top_val, top_ind = torch.topk(loss, 4, largest=False, dim=0)
        # return affine_mats[top_ind]

    def _calculate_cm(self, images):
        """Calculates the center of mass of the images.
        
        Args:
            images (torch.Tensor): The images. Shape (N, C, x, y, z)
            
        Returns:
            torch.Tensor: The center of mass of the images. Shape (N, 3)
        """
        # Create the grid
        grid = torch.stack(
            torch.meshgrid(
                torch.arange(images.shape[2], device=images.device) * self.pix_dim[0],
                torch.arange(images.shape[3], device=images.device) * self.pix_dim[1],
                torch.arange(images.shape[4], device=images.device) * self.pix_dim[2],
                indexing="ij",
            ))
        grid = grid.type(torch.float32)[None, ...]
        # Calculate the center of mass
        cm = torch.sum(images * grid, dim=(2, 3, 4)) / torch.sum(images, dim=(2, 3, 4))
        return cm

    def _calculate_moments(self, images, cm):
        """Calculates the moments of the images.
        
        Args:
            images (torch.Tensor): The images. Shape (N, C, x, y, z)
            cm (torch.Tensor): The center of mass of the images. Shape (N, 3)
            
        Returns:
            torch.Tensor: The moments of the images. Shape (N, 3)
        """
        # Create the grid
        grid = torch.stack(
            torch.meshgrid(
                torch.arange(images.shape[2], device=images.device) * self.pix_dim[0],
                torch.arange(images.shape[3], device=images.device) * self.pix_dim[1],
                torch.arange(images.shape[4], device=images.device) * self.pix_dim[2],
                indexing="ij",
            ))
        grid = grid.type(torch.float32)[None, ...]
        # Calculate the moments
        moments = torch.sqrt(
            torch.sum(images * (grid - cm[:, :, None, None, None])**2, dim=(2, 3, 4)) /
            torch.sum(images**2, dim=(2, 3, 4)))

        print(moments)

        return moments

    def _get_uniform_rotations(self, n_angles):
        """
        Taken from: https://www.blopig.com/blog/2021/08/uniformly-sampled-3d-rotation-matrices/
        Apply a random rotation matrix in 3D, with a distribution uniform over the
        sphere.
        Algorithm taken from "Fast Random Rotation Matrices" (James Avro, 1992):
        https://doi.org/10.1016/B978-0-08-050755-2.50034-8
        Arg:
            x: vector or set of vectors with dimension (n, 3), where n is the
                number of vectors
        Returns:
            Array of shape (n, 3) containing the randomly rotated vectors of x,
            about the mean coordinate of x.
        """
        # There are two random variables in [0, 1) here (naming is same as paper)

        x1 = torch.rand(n_angles, device=self.device)
        x2 = torch.rand(n_angles, device=self.device)
        x3 = torch.rand(n_angles, device=self.device)

        x2 = 2 * np.pi * x2

        print(f"x1: {x1}")
        print(f"x2: {x2}")
        print(f"x3: {x3}")

        # Generate random rotation matrix about the z axis.
        R = torch.eye(3).repeat(x1.shape[0], 1, 1).to(x1.device)

        R[:, 0, 0] = R[:, 1, 1] = torch.cos(2 * torch.pi * x1)
        R[:, 0, 1] = -torch.sin(2 * torch.pi * x1)
        R[:, 1, 0] = -R[:, 0, 1]

        # Rotation of all points around x axis using matrix
        v = torch.stack([torch.cos(x2) * torch.sqrt(x3), torch.sin(x2) * torch.sqrt(x3), torch.sqrt(1 - x3)], dim=1)
        H = torch.eye(3, device=self.device).repeat(x1.shape[0], 1, 1) - (2 * torch.einsum("ni, nj -> nij", v, v))
        M = -torch.einsum("nij, njk -> nik", H, R)
        return M

    def _rotate_affine_mats(self, affine_mats, rot_mats, cm):
        """Rotates the affine matrices around the center of mass.

        Args:
            affine_mats (torch.Tensor): The affine matrices. Shape (N, 3, 4)
            rot_mats (torch.Tensor): The rotation matrices. Shape (N, 3, 3)
            cm (torch.Tensor): The center of mass of the images. Shape (N, 3)

        Returns:
            torch.Tensor: The rotated affine matrices. Shape (N, 3, 4)
        """

        # Move the center of mass to the origin
        #affine_mats[:, :3, 3] = affine_mats[:, :3, 3] - cm
        # Rotate the affine matrices
        affine_mats[:, :3, :3] = torch.einsum("nij, njk -> nik", rot_mats, affine_mats[:, :3, :3])
        affine_mats[:, :3, 3] = torch.einsum("nij, nj -> ni", rot_mats, affine_mats[:, :3, 3])
        # Move the center of mass back
        #affine_mats[:, :3, 3] = affine_mats[:, :3, 3] + cm
        return affine_mats

    def combine_affine_matrices(self, mat1, mat2):
        """
        Assumes the matrices are in the shape (N, 3, 4)
        """
        mat1 = torch.concat([mat1, torch.zeros_like(mat1[:, :1, :])], dim=1)
        mat1[:, 3, 3] = 1

        mat2 = torch.concat([mat2, torch.zeros_like(mat2[:, :1, :])], dim=1)
        mat2[:, 3, 3] = 1

        mat = torch.einsum("nij, njk -> nik", mat1, mat2)
        return mat[:, :3, :4]

    def _get_affine_grid(self, rotation=None, translation=None, matrix=None, type='img', cm=None):
        """
        Computes the affine grid from rotation and translation
        """

        if matrix is None:
            matrix = self._get_homogeneous_transformation_matrix(rotation, translation)

        if type == 'img':
            grid = self.sp_tr_img.grid.detach().clone().repeat(matrix.shape[0], 1, 1, 1, 1)
        else:
            grid = self.sp_tr.grid.detach().clone().repeat(matrix.shape[0], 1, 1, 1, 1)

        # Grid is in the shape (N, 3, H, W, D)
        # Matrix is in the shape (N, 3, 4)
        # Move grid to center of image
        #print(f"Max of grid: {torch.max(grid, dim=1)}")
        #print(f"Estiamted grid max: {self.image_size * self.pix_dim}")
        # grid[:, :3, :] = grid[:, :3, :] - torch.tensor(
        #     self.image_size * self.pix_dim, device=self.device)[None, :, None, None, None] / 2
        # Rotate around the center of mass

        # Add [0, 0, 0, 1]

        cm = torch.tensor(self.sp_tr_img.size_mm) / 2

        cm_matrix = torch.zeros_like(matrix)
        cm_matrix[:, :3, :3] = torch.eye(3, device=self.device)
        cm_matrix[:, :3, 3] = cm

        matrix = self.combine_affine_matrices(cm_matrix, matrix)
        cm_matrix[:, :3, 3] = -cm
        matrix = self.combine_affine_matrices(matrix, cm_matrix)

        return torch.einsum("nij, njhwd -> nihwd", matrix[:, :3, :3], grid) + matrix[:, :3, 3, None, None, None]

    def _affine_transform(self, images, matrix, cm=None):
        return self.sp_tr_img(images, self._get_affine_grid(matrix=matrix, cm=cm), displacement=False)
