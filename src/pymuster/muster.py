"""Multi Session Temporal Registration.

This module is the core of Multi Session Temporal Registration (MUSTER).
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np

from alive_progress import alive_bar

from .core import field_calculus
from .core import losses
from .core import utils

import warnings
from typing import Union


class StageRegistration:
    """ Register a longitudinal series of images using a series of deformation fields using a single resolution.

    The images must be in the same space and must be pre-registered using an rigid or affine transformation.

    The deformation between two consectutive timepoints is represented by a deformation field. These fields are 
    generated by integrating a vector field, also known as the deformation flow. Various methods for 
    integration are supported:
    
        * ``integration_method="ss"``: Use scaling and squaring to integrate stationary vector fields. (Default)
        * ``integration_method="euler"``: Use euler integration for stationary vector fields.
        * ``integration_method="rk4"``: Use Runge-Kutta 4 integration for stationary vector field.
        * ``integration_method="euler_time"``: Use euler integration for time varying vector fields.
        * ``integration_method="rk4_time"``: Use Runge-Kutta 4 integration for time varying vector fields.

    For non-consecutive timepoints, the deformation field can be composed in two ways:
    
        * ``field_composition_method="interpolate"``: Use interpolation to compose the intermediate deformation fields. (Default)
        * ``field_composition_method="flow_continuation"``: Continuate the deformation field by integrating the intermediate deformation flows.

    Supported image similarity metrics include:
    
        * ``img_similarity_metric="NCC"``: Normalized local cross correlation between the two images as loss. The local neighborhood is a cube of size with side length ``img_similarity_spatial_size``.
        * ``img_similarity_metric="L2"``: L2 norm of the difference between the two images as loss. 
        * ``img_similarity_metric="NCCS"``: Use a sobel filter to compute the gradient of the two images, and use the normalized local cross correlation between the two gradients as loss. The local window is a cube of size with side length ``3``.
        * ``img_similarity_metric="WNCC"``: Normalized local cross correlation between the two images as loss where each local neighborhood is weighted by the cross standard deviation of the two images.The local neighborhood is a cube of size with side length ``img_similarity_spatial_size``.
        * ``img_similarity_metric="GaussNCC"``: Normalized local cross correlation between the two images as loss where each local neighborhood is weighted by a gaussian filter. The standard deviation of the gaussian filter is given by ``img_similarity_spatial_size``.
        * ``img_similarity_metric="Fourier"``: Use the Fourier tranform to compute a filtered gradient of each images, and compute the global normalized cross correlation between the two filtered gradients as loss. The standard deviation of the gaussian filter is ``img_similarity_spatial_size``.


    Args:
        img_size (tuple): The size of the input images in the format ``(x, y, z)``.
        pix_dim (tuple): The pixel dimensions of the input images in the format ``(dx, dy, dz)``. Default is ``(1, 1, 1)``.
        deform_res_scale (int): The resolution scale of the deformation field. The deformation field will have a 
            resolution of img_size/deform_res_scale. Default is ``1``.
        device (str): The device to use for computation. ('cpu' or 'cuda:n' or torch.device object). Default is ``"cpu"``.
        num_iterations (int): Number of optimization iterations. Default is ``100``.
        spatial_smoothness_penalty (float): Weight for spatial smoothness in loss function. Default is ``1.0``.
        temporal_smoothness_penalty (float): Weight for temporal smoothness in loss function. Default is ``1.0``.
        invertability_penalty (float): Weight for invertibility penalty in loss function. Default is ``1.0``.
        l2_penalty (float): Weight for L2 penalty on the deformation flow. Default is ``0.0``.
        smoothing_sigma (float): Sigma for Gaussian smoothing of deformation flows. Default is ``1.0``.
        mode (str): The mode for interpolation of the images. ('nearest', 'bilinear', or 'bicubic'). Default is ``"bilinear"``.
        integration_steps (int): The number of integration steps for integrating deformation flow. Default is ``7``.
        integration_method (str): The method for integrating deformation field.
            ('ss', 'euler', 'rk4', 'euler_time', or 'rk4'). Default is ``"ss"``.
        field_composition_method (str): The method for composing deformation fields.
            ('interpolate' or 'flow_continuation'). Default is ``"interpolate"``.
        affine_adjustment (str): The method for adjusting the affine transformation. ('none', 'rigid', or 'affine'). Default is ``"none"``.
        learning_rate (float): Learning rate for optimizer. Default is ``1e-3``.
        betas (tuple): The beta coefficients for Adam optimizer. Default is ``(0.9, 0.999)``.
        tol (float): Tolerance for optimization convergence. Default is ``1e-4``.
        img_similarity_metric (str): The image similarity metric ('NCC', 'L2', 'NCCS', 'WNCC', 'GaussNCC', or 'Fourier').
            Default is ``"NCC"``.
        img_similarity_spatial_size (int/float): Size or standard deviation of the local neighborhood for the image 
            similarity metric. Default is ``3``.
        verbose (bool): Flag for printing progress during optimization. Default is ``True``.
    """

    def __init__(
        self,
        img_size: tuple,
        pix_dim: tuple = (1, 1, 1),
        deform_res_scale: int = 1,
        device: str = "cpu",
        num_iterations: int = 100,
        spatial_smoothness_penalty: float = 0.0,
        temporal_smoothness_penalty: float = 0.0,
        invertability_penalty: float = 0.0,
        l2_penalty: float = 0.0,
        smoothing_sigma: float = 0.0,
        interpolation_mode: str = "bilinear",
        integration_steps: int = 7,
        integration_method: str = "ss",
        field_composition_method: str = "interpolate",
        affine_adjustment: str = "none",
        learning_rate: float = 1e-3,
        betas: tuple = (0.9, 0.999),
        tol: float = 1e-4,
        img_similarity_metric: str = "NCC",
        img_similarity_spatial_size: Union[int, float] = 3,
        img_similarity_scale_invariant: bool = True,
        verbose: bool = True,
    ):
        # Valid values for integration_method
        valid_integration_methods = ["ss", "euler", "rk4", "euler_time", "rk4_time"]
        if integration_method not in valid_integration_methods:
            raise ValueError(f"integration_method must be one of {valid_integration_methods}")

        # Valid values for field_composition_method
        valid_field_composition_methods = ["interpolate", "flow_continuation"]
        if field_composition_method not in valid_field_composition_methods:
            raise ValueError(f"field_composition_method must be one of {valid_field_composition_methods}")

        # flow_continuation can not be combined with ss
        if field_composition_method == "flow_continuation" and integration_method == "ss":
            raise ValueError("flow_continuation can not be combined with ss")

        # Valid values for img_similarity_metric
        valid_img_similarity_metrics = ["NCC", "L2", "NCCS", "Fourier", "WNCC", "GaussNCC", "VELLN", "VELLNGAUSS"]
        if img_similarity_metric not in valid_img_similarity_metrics:
            raise ValueError(f"img_similarity_metric must be one of {valid_img_similarity_metrics}")

        # Valid values for mode
        valid_modes = ["nearest", "bilinear", "bicubic"]
        if interpolation_mode not in valid_modes:
            raise ValueError(f"mode must be one of {valid_modes}")

        # Check the dimensions of the image
        if len(img_size) != 3:
            raise ValueError("img_size must be a tuple of length 3")

        # Check the dimensions of the pixel dimensions
        if len(pix_dim) != 3:
            raise ValueError("pix_dim must be a tuple of length 3")

        self.device = torch.device(device)

        self.num_iterations = num_iterations
        self.spatial_smoothness_penalty = spatial_smoothness_penalty
        self.temporal_smoothness_penalty = temporal_smoothness_penalty
        self.l2_penalty = l2_penalty
        self.smoothing_sigma = smoothing_sigma
        self.integration_steps = integration_steps
        self.integration_method = integration_method
        self.field_composition_method = field_composition_method

        self.affine_adjustment = affine_adjustment
        self.betas = betas
        self.learning_rate = learning_rate
        self.img_size = img_size
        self.deform_res_scale = deform_res_scale
        self.verbose = verbose
        self.tol = tol
        self.img_similarity_metric = img_similarity_metric
        self.pix_dim = pix_dim
        self.invertability_penalty = invertability_penalty
        self.img_similarity_spatial_size = img_similarity_spatial_size

        self.deform_size = [int(s // self.deform_res_scale) for s in self.img_size]
        self.deform_pix_dim = [s * self.deform_res_scale for s in self.pix_dim]
        self.interpolation_mode = interpolation_mode

        self.sp_tr = SpatialTransformer(
            self.deform_size,
            self.deform_pix_dim,
            mode=interpolation_mode,
            padding_mode="border",
        ).to(self.device)

        if self.img_similarity_metric == "MSE" or self.img_similarity_metric == 'L2':
            self.img_sim_metric_fnc = losses.MSE()
        elif self.img_similarity_metric == "NCC":
            self.img_sim_metric_fnc = losses.NCC(
                self.img_similarity_spatial_size, scale_invariant=img_similarity_scale_invariant)
        elif self.img_similarity_metric == "GaussNCC":
            if self.img_similarity_spatial_size < self.pix_dim[0] + 1:
                warnings.warn(
                    f"img_similarity_spatial_size ({self.img_similarity_spatial_size}) is smaller than the pixel size ({self.pix_dim[0] + 1})."
                    " This may cause the gaussian filter to return NaNs."
                    " Setting img_similarity_spatial_size to the pixel size + 1.")
                self.img_similarity_spatial_size = self.pix_dim[0] + 1
            self.img_sim_metric_fnc = losses.GaussNCC(
                self.img_similarity_spatial_size,
                self.img_size,
                self.pix_dim,
                scale_invariant=img_similarity_scale_invariant)
        elif self.img_similarity_metric == "NCCS":
            self.img_sim_metric_fnc = losses.NCCS(self.img_similarity_spatial_size)
        elif self.img_similarity_metric == "VELLN":
            self.img_sim_metric_fnc = losses.VELLN(self.img_similarity_spatial_size, kernal_type='window')
        elif self.img_similarity_metric == "VELLNGAUSS":
            self.img_sim_metric_fnc = losses.VELLN(
                self.img_similarity_spatial_size, kernal_type='gaussian', pix_dim=self.pix_dim, img_size=self.img_size)

        self.sp_tr_img = SpatialTransformer(
            self.img_size, pix_dim, mode=interpolation_mode, padding_mode="border").to(self.device)

    def fit(
        self,
        images,
        inital_deform_flow=None,
        inital_sigmas=None,
        timepoints=None,
        initial_rotations=None,
        initial_translations=None,
        initial_affine=None,
        masks=None,
    ):
        """Fits the deformation flow to the images.

        Args:
            images (torch.Tensor or np.ndarray): shape ``(N, channels, x, y, z)``
            inital_deform_flow (torch.Tensor or np.ndarray): shape ``(N-1, 3, x, y, z)``
            timepoints (torch.Tensor or np.ndarray): shape ``(N,)``
                The timepoints of the images. Used to adjust the temporal
                penalties. May be None, in which case the timepoints are
                assumed to be equally spaced.

        Returns:
            out (dict): Dictionary containing the deformation flow and optionally the rotations and translations.
        """

        self.n_img = images.shape[0]  # Number of images
        if self.integration_method in ['ss', 'euler', 'rk4']:
            self.n_flow = self.n_img - 1  # Number of deformations flow fields
        else:
            self.n_flow = self.n_img

        # Set device
        device = self.device

        images = self._convert_to_torch(images)

        if masks is None:
            masks = torch.ones_like(images)

        # The time difference between the images
        if timepoints is None:
            delta_times = torch.ones((self.n_img - 1, 1, 1, 1, 1), device=device)
        else:
            timepoints = self._convert_to_torch(timepoints)
            delta_times = timepoints[1:] - timepoints[:-1]
            delta_times = delta_times[:, None, None, None, None]

        # The deformation flow
        if inital_deform_flow is None:
            deform_flow = nn.parameter.Parameter(torch.zeros((self.n_flow, 3, *self.deform_size), device=device))
        else:
            inital_deform_flow = self._convert_to_torch(inital_deform_flow)
            deform_flow = nn.parameter.Parameter(inital_deform_flow.clone().to(device))

        param_groups = [{"params": [deform_flow], "lr": self.learning_rate}]

        # Parameters for rigid ajustment of the images
        if self.affine_adjustment == "rigid":
            if initial_rotations is None:
                rotation_params = nn.Parameter(torch.zeros((self.n_img - 1, 3), device=device))
            else:
                initial_rotations = self._convert_to_torch(initial_rotations[1:])
                rotation_params = nn.Parameter(initial_rotations.clone().to(device))

            if initial_translations is None:
                translation_params = nn.Parameter(torch.zeros((self.n_img - 1, 3), device=device))
            else:
                initial_translations = self._convert_to_torch(initial_translations[1:])
                translation_params = nn.Parameter(initial_translations.clone().to(device))

            param_groups.append({
                "params": [rotation_params, translation_params],
                "lr": self.learning_rate * 0.0001,
            })

        if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
            log_sigmas = nn.parameter.Parameter(torch.ones((self.n_img, 1, 1, 1, 1), device=device) * (-1.0))
            param_groups.append({"params": [log_sigmas], "lr": self.learning_rate * 10})

        optimizer = torch.optim.Adam(param_groups, lr=self.learning_rate, betas=self.betas)
        scaler = torch.amp.GradScaler()

        # Cosine annealing with linear warmup
        warmup_steps = int(self.num_iterations * 0.2)
        scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1 / (warmup_steps) * (step + 1))
        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=int(self.num_iterations - warmup_steps))
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer, schedulers=[scheduler1, scheduler2], milestones=[warmup_steps])

        # Object for keeping track of convergence of the optimization process
        running_mean_var = utils.RunningMeanVar()

        use_autocast = False  #torch.cuda.is_available() and (str(device) != "cpu")

        with alive_bar(
                self.num_iterations,
                force_tty=True,
                max_cols=130,
                dual_line=True,
                title_length=50,
                elapsed="({elapsed})",
                stats="(eta: {eta})",
                disable=not self.verbose,
        ) as bar:
            for iteration_idx in range(self.num_iterations):
                optimizer.zero_grad()

                with torch.amp.autocast(device_type="cuda", enabled=use_autocast):
                    if self.affine_adjustment == "rigid":
                        # Add the identity transformation to first timepoint
                        rots = torch.cat((torch.zeros((1, 3), device=device), rotation_params), dim=0)
                        trans = torch.cat((torch.zeros((1, 3), device=device), translation_params), dim=0)
                        tran_grid = self._get_affine_grid(rots, trans)

                    else:
                        rots = torch.zeros((self.n_img, 3), device=device)
                        trans = torch.zeros((self.n_img, 3), device=device)
                        tran_grid = self._get_affine_grid(rots, trans)

                    loss = 0
                    image_loss = 0
                    inv_loss = 0

                    # The cumulative deformation fields
                    cum_deform_field_fwd = self._intergate_flow(deform_flow, direction="fwd")  # mm/step
                    cum_deform_field_bwd = self._intergate_flow(deform_flow, direction="bwd")  # mm/step

                    # The consecutive deformation fields
                    deform_field_fwd = cum_deform_field_fwd
                    deform_field_bwd = cum_deform_field_bwd

                    # Regularize the deformation field
                    if self.integration_method == "ss":
                        loss += self._spatial_continuity_loss(deform_flow / torch.sqrt(delta_times))
                    else:
                        loss += self._spatial_continuity_loss(deform_flow)

                    loss += self.l2_penalty * torch.mean((deform_flow / torch.sqrt(delta_times))**2)

                    if self.n_img > 2:
                        loss += (
                            self.temporal_smoothness_penalty * self._temperal_continuity_loss(deform_flow, delta_times))

                    # Loop over the time difference between the images starting with consecutive images and ending with the
                    # first and last image
                    for delta_timestep in range(self.n_img - 1):
                        if delta_timestep > 0:
                            (cum_deform_field_fwd, cum_deform_field_bwd) = self._combine_deform_fields(
                                cum_deform_field_fwd,
                                cum_deform_field_bwd,
                                deform_flow,
                                delta_timestep,
                                deform_field_fwd,
                                deform_field_bwd,
                            )

                        # Interpolate the cum_deform_fields to the resolution of the images
                        if np.any(np.array(self.img_size) != np.array(self.deform_size)):
                            cum_deform_field_fwd_interp = nn.functional.interpolate(
                                cum_deform_field_fwd,
                                size=self.img_size,
                                mode="trilinear",
                                align_corners=True,
                            )
                            cum_deform_field_bwd_interp = nn.functional.interpolate(
                                cum_deform_field_bwd,
                                size=self.img_size,
                                mode="trilinear",
                                align_corners=True,
                            )
                        else:
                            cum_deform_field_fwd_interp = cum_deform_field_fwd
                            cum_deform_field_bwd_interp = cum_deform_field_bwd

                        warped_images = self.sp_tr_img(
                            images[:-delta_timestep - 1, ...],
                            cum_deform_field_fwd_interp + tran_grid[:-delta_timestep - 1, ...],
                            displacement=False)
                        warped_maskes = self.sp_tr_img(
                            masks[:-delta_timestep - 1, ...],
                            cum_deform_field_fwd_interp + tran_grid[:-delta_timestep - 1, ...],
                            displacement=False)

                        if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
                            image_loss += self.img_sim_metric_fnc.loss(
                                images[delta_timestep + 1:, ...],
                                warped_images,
                                mask=warped_maskes * masks[delta_timestep + 1:, ...],
                                sigma_true=log_sigmas[delta_timestep + 1:, ...],
                                sigma_pred=log_sigmas[:-delta_timestep - 1, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        else:
                            image_loss += self.img_sim_metric_fnc.loss(
                                images[delta_timestep + 1:, ...],
                                warped_images,
                                mask=warped_maskes * masks[delta_timestep + 1:, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        warped_images = self.sp_tr_img(
                            images[delta_timestep + 1:, ...],
                            cum_deform_field_bwd_interp + tran_grid[delta_timestep + 1:, ...],
                            displacement=False)
                        warped_maskes = self.sp_tr_img(
                            masks[delta_timestep + 1:, ...],
                            cum_deform_field_bwd_interp + tran_grid[delta_timestep + 1:, ...],
                            displacement=False)

                        if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
                            image_loss += self.img_sim_metric_fnc.loss(
                                images[:-delta_timestep - 1, ...],
                                warped_images,
                                mask=warped_maskes * masks[:-delta_timestep - 1, ...],
                                sigma_true=log_sigmas[:-delta_timestep - 1, ...],
                                sigma_pred=log_sigmas[delta_timestep + 1:, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)
                        else:
                            image_loss += self.img_sim_metric_fnc.loss(
                                images[:-delta_timestep - 1, ...],
                                warped_images,
                                mask=warped_maskes * masks[:-delta_timestep - 1, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        if self.invertability_penalty > 0:
                            inv_loss += self.invertability_penalty * self._invertability_loss(
                                cum_deform_field_fwd, cum_deform_field_bwd) * (
                                    self.n_img - 1 - delta_timestep)

                    # Normalize with respect to the number of images
                    image_loss /= (self.n_img - 1) * self.n_img
                    inv_loss /= (self.n_img - 1) * self.n_img

                    # Combine the losses
                    loss += image_loss + inv_loss

                if use_autocast:
                    # Propagate the loss and update the parameters
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    optimizer.step()

                # Ignore the warning from the scheduler
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    scheduler.step()

                bar()
                bar.title(f"\tLoss: {loss.item():.4e}, Image Loss: {image_loss.item():.4e}")
                bar.title_length = 200

                running_mean_var.add(loss.item())

                if self.verbose:
                    if iteration_idx == self.num_iterations - 1:
                        bar.title(f"\tMax iterations reached")

                # if (iteration_idx > 20 and np.sqrt(running_mean_var.var()) / running_mean_var.mean() < self.tol):
                #     bar.title(f"\tConvergence reached")
                #     break

        optimizer.zero_grad()

        if self.affine_adjustment == "rigid":
            return_img = self.sp_tr_img(images, tran_grid, displacement=False)
        else:
            return_img = self.sp_tr_img(images, tran_grid, displacement=False)

        return_dict = {
            "deform_flow": deform_flow.detach().cpu().numpy(),
            "images": return_img.detach().cpu().numpy(),
        }
        if self.affine_adjustment == "rigid":
            return_dict["rotations"] = rotation_params.detach().cpu().numpy()
            return_dict["translations"] = translation_params.detach().cpu().numpy()

        return return_dict

    def _get_homogeneous_transformation_matrix(self, rotation, translation):
        """Computes the affine matrix from rotation and translation
        """
        Rx = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Rx[:, [1, 2], [1, 2]] = torch.cos(rotation[:, 0])[:, None]
        Rx[:, 1, 2] = -torch.sin(rotation[:, 0])
        Rx[:, 2, 1] = torch.sin(rotation[:, 0])

        Ry = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Ry[:, [0, 2], [0, 2]] = torch.cos(rotation[:, 1])[:, None]
        Ry[:, 0, 2] = torch.sin(rotation[:, 1])
        Ry[:, 2, 0] = -torch.sin(rotation[:, 1])

        Rz = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Rz[:, [0, 1], [0, 1]] = torch.cos(rotation[:, 2])[:, None]
        Rz[:, 0, 1] = -torch.sin(rotation[:, 2])
        Rz[:, 1, 0] = torch.sin(rotation[:, 2])

        R = torch.matmul(Rz, torch.matmul(Ry, Rx))
        matrix = torch.cat([R, translation[:, :, None]], dim=2)
        return matrix

    def _get_affine_grid(self, rotation=None, translation=None, matrix=None, type='img'):
        """
        Computes the affine grid from rotation and translation
        """

        if matrix is None:
            matrix = self._get_homogeneous_transformation_matrix(rotation, translation)

        if type == 'img':
            grid = self.sp_tr_img.grid
        else:
            grid = self.sp_tr.grid

        # Grid is in the shape (N, 3, H, W, D)
        # Matrix is in the shape (N, 3, 4)
        grid = (torch.einsum("nij, njhwd -> nihwd", matrix[:, :3, :3], grid) + matrix[:, :3, 3, None, None, None])
        return grid

    def _invertability_loss(self, deform_field_fwd, deform_field_bwd):
        method = "ICON"  # 'ICON' or 'gradICON'
        if method == "Gamma":
            loss = 0
            for deform_field in [deform_field_fwd, deform_field_bwd]:
                J = field_calculus.jacobian(deform_field, pix_dim=self.deform_pix_dim)
                margin = 0
                loss = 0
                loss += torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**
                                   2) * self.spatial_smoothness_penalty

                det_J = field_calculus.determinant(J +
                                                   torch.eye(3, device=self.device)[None, :, :, None, None, None])[:,
                                                                                                                   None,
                                                                                                                   ...]

                sigma_det_J = 1.0
                theta = 0.5 * (np.sqrt(1 + 4 * sigma_det_J**2) - 1)
                k = sigma_det_J**2 / theta**2
                loss += torch.mean(-(k - 1) * torch.log(det_J) + det_J / theta - 1 / theta)
            return loss
        else:
            grid = self.sp_tr.grid

            deform_pix_dim = torch.tensor(self.deform_pix_dim, device=self.device)[None, :, None, None, None]

            fwd_loss = (self.sp_tr(deform_field_bwd + grid, deform_field_fwd) - grid) / deform_pix_dim
            bwd_loss = (self.sp_tr(deform_field_fwd + grid, deform_field_bwd) - grid) / deform_pix_dim

            if method == "ICON":
                return torch.mean(fwd_loss**2 + bwd_loss**2)
            elif method == "gradICON":
                J = field_calculus.jacobian(torch.cat([fwd_loss, bwd_loss], dim=0), self.deform_pix_dim)
                margin = 2
                J = J[:, :, :, margin:-margin, margin:-margin, margin:-margin]
                return torch.mean((J)**2)

    def _spatial_continuity_loss(self, deform_flow):

        J = field_calculus.jacobian(deform_flow, pix_dim=self.deform_pix_dim)
        margin = 0
        # Normal distribution prior over the Jacobian
        loss = torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**
                          2) * self.spatial_smoothness_penalty

        return loss

    def _temperal_continuity_loss(self, deform_flow, delta_times):
        if (self.integration_method == "ss") or (self.integration_method == "euler") or (self.integration_method
                                                                                         == "rk4"):
            return torch.mean(
                ((deform_flow[1:, ...] / delta_times[1:, ...] - deform_flow[:-1, ...] / delta_times[:-1, ...])**2))
        else:
            return torch.mean(((deform_flow[1:, ...] - deform_flow[:-1, ...]) / delta_times)**2)

    def _intergate_flow(self, deform_flow, direction, target_grid=None):
        """
        Integrates a vector field via scaling and squaring.
        Args:
            deform_flow: The vector field to integrate. Shape (N, 3, x, y, z)
            direction: The direction to integrate in. ('fwd' or 'bwd')
            target_grid: The grid to start the integration from. If None, the grid will be set to the identity transformation.
        """
        super().__init__()
        nsteps = self.integration_steps
        method = self.integration_method

        if target_grid is None:
            if method in ["ss", "euler", "rk4"]:
                target_grid = (self.sp_tr.grid.detach().clone().repeat(deform_flow.shape[0], 1, 1, 1, 1))
            else:
                target_grid = (self.sp_tr.grid.detach().clone().repeat(deform_flow.shape[0] - 1, 1, 1, 1, 1))

        if self.smoothing_sigma > 0:
            deform_flow = self._gaussian_smooth(deform_flow, self.smoothing_sigma)

        if method == "ss":
            # Integrate a constant vector field with scaling and squaring
            scale = 1.0 / (2**nsteps)
            if direction in ["backward", "bwd"]:
                scale = -scale
            vec = deform_flow * scale
            for _ in range(nsteps):
                vec = vec + self.sp_tr(vec, vec)
            return vec

        elif method == "euler":
            # Integrate a constant vector field with euler integration
            vec = torch.zeros_like(deform_flow)
            dt = 1.0 / nsteps
            if direction in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                vec = vec + self.sp_tr(deform_flow * dt, vec + target_grid, displacement=False)
            return vec
        elif method == "euler_time":
            # Integrate a constant vector field with euler integration
            vec = torch.zeros(
                (deform_flow.shape[0] - 1, 3, *self.deform_size),
                device=self.device,
            )
            dt = 1.0 / nsteps
            if direction in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                if direction in ["backward", "bwd"]:
                    flow = (1 - i / nsteps) * deform_flow[1:, ...] + (i / nsteps) * deform_flow[:-1, ...]
                else:
                    flow = (1 - i / nsteps) * deform_flow[:-1, ...] + (i / nsteps) * deform_flow[1:, ...]

                vec = vec + self.sp_tr(flow * dt, vec + target_grid, displacement=False)
            return vec

        elif method == "rk4":
            vec = torch.zeros_like(deform_flow)
            dt = 1.0 / nsteps
            if direction in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                k1 = self.sp_tr(deform_flow * dt, vec + target_grid, displacement=False)
                k2 = self.sp_tr(
                    deform_flow * dt,
                    vec + target_grid + k1 * 0.5,
                    displacement=False,
                )
                k3 = self.sp_tr(
                    deform_flow * dt,
                    vec + target_grid + k2 * 0.5,
                    displacement=False,
                )
                k4 = self.sp_tr(deform_flow * dt, vec + target_grid + k3, displacement=False)
                vec = vec + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0
            return vec
        elif method == "rk4_time":
            vec = torch.zeros(
                (deform_flow.shape[0] - 1, 3, *self.deform_size),
                device=self.device,
            )
            dt = 1.0 / nsteps
            if direction in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                if direction in ["backward", "bwd"]:
                    flow = (1 - i / nsteps) * deform_flow[1:, ...] + (i / nsteps) * deform_flow[:-1, ...]
                else:
                    flow = (1 - i / nsteps) * deform_flow[:-1, ...] + (i / nsteps) * deform_flow[1:, ...]
                k1 = self.sp_tr(flow * dt, vec + target_grid, displacement=False)
                k2 = self.sp_tr(flow * dt, vec + target_grid + k1 * 0.5, displacement=False)
                k3 = self.sp_tr(flow * dt, vec + target_grid + k2 * 0.5, displacement=False)
                k4 = self.sp_tr(flow * dt, vec + target_grid + k3, displacement=False)
                vec = vec + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0
            return vec
        elif method == "trapz":
            # A secret method that also works
            # Find the vector field that transforms the midpoint grid to the target grid
            k = 0.5
            midpoint_grid = target_grid
            landing_grid = target_grid
            if direction in ["backward", "bwd"]:
                direction = -1
            elif direction in ["forward", "fwd"]:
                direction = 1

            for i in range(nsteps):
                midpoint_grid = midpoint_grid - \
                    k * (landing_grid - target_grid)
                # The boundary condition is that the deformation field continue outside the grid,
                landing_grid = (
                    self.sp_tr(-deform_flow * 0.5 * direction, midpoint_grid, displacement=False) + midpoint_grid)

            return direction * self.sp_tr(
                deform_flow,
                midpoint_grid - k * (landing_grid - target_grid),
                displacement=False,
            )

    def _combine_deform_fields(
        self,
        cum_deform_field_fwd,
        cum_deform_field_bwd,
        deform_flow,
        delta_timestep,
        deform_field_fwd,
        deform_field_bwd,
    ):
        """Adds the integral of the deformation flow to the cumulative deformation fields.
        
            If ``field_composition_method`` is set to ``interpolate``, the composition is done by interpolation of 
            the consecutive deformation fields with the cumulative deformation fields.
            If ``field_composition_method`` is set to ``flow_continuation``, the composition is done by integrating 
            the deformation flow from the enpoint of the cumulative deformation fields.
            
            ``delta_timestep`` gives the number of times the cumulative deformation fields has been integrated.
            
            Args:
                cum_deform_field_fwd (torch.Tensor): Cumulative forward deformation field.
                cum_deform_field_bwd (torch.Tensor): Cumulative backward deformation field.
                deform_flow (torch.Tensor): Deformation flow.
                delta_timestep (int): The number of timesteps the cumulative deformation fields has been integrated.
                deform_field_fwd (torch.Tensor): Consecutive forward deformation field.
                deform_field_bwd (torch.Tensor): Consecutive backward deformation field.

            Returns:
                tuple: A tuple containing:

                    - **cum_deform_field_fwd** (torch.Tensor): Cumulative forward deformation field.
                    - **cum_deform_field_bwd** (torch.Tensor): Cumulative backward deformation field.
            """
        Nd = deform_field_fwd.shape[0]

        method = self.field_composition_method

        fwd_grid = cum_deform_field_fwd[:-1, ...] + self.sp_tr.grid
        bwd_grid = cum_deform_field_bwd[1:, ...] + self.sp_tr.grid

        if method == "interpolate":
            cum_deform_field_fwd = cum_deform_field_fwd[:-1, ...] + self.sp_tr(
                deform_field_fwd[delta_timestep:, ...],
                fwd_grid,
                displacement=False,
            )
            cum_deform_field_bwd = cum_deform_field_bwd[1:, ...] + self.sp_tr(
                deform_field_bwd[:Nd - delta_timestep, ...],
                bwd_grid,
                displacement=False,
            )
        elif method == "flow_continuation":
            Nd = deform_flow.shape[0]
            cum_deform_field_fwd = cum_deform_field_fwd[:-1, ...] + self._intergate_flow(
                deform_flow[delta_timestep:, ...], direction="fwd", target_grid=fwd_grid)
            cum_deform_field_bwd = cum_deform_field_bwd[1:, ...] + self._intergate_flow(
                deform_flow[:Nd - delta_timestep, ...],
                direction="bwd",
                target_grid=bwd_grid,
            )
        else:
            raise ValueError("field_composition_method must be either 'interpolation' or 'flow_continuity'")
        return cum_deform_field_fwd, cum_deform_field_bwd

    def _gaussian_smooth(self, field, sigma):
        """Smooths the deformation field with a Gaussian filter in the Fourier domain.

        Args:
            field (torch.Tensor): The deformation field to be smoothed.
            sigma (float): The standard deviation of the Gaussian filter.

        Returns:
            torch.Tensor: The smoothed deformation field.
        """
        v_fft = torch.fft.fftn(field, dim=[2, 3, 4])

        freqx = torch.fft.fftfreq(field.shape[2], d=self.deform_pix_dim[0]).to(self.device)
        freqy = torch.fft.fftfreq(field.shape[3], d=self.deform_pix_dim[1]).to(self.device)
        freqz = torch.fft.fftfreq(field.shape[4], d=self.deform_pix_dim[2]).to(self.device)

        freqx, freqy, freqz = torch.meshgrid(freqx, freqy, freqz, indexing="ij")
        omega_f = 1 / (2 * torch.pi * sigma)
        filter_response = torch.exp(-1 / 2 * ((freqx**2 + freqy**2 + freqz**2) / omega_f**2))
        v_fft = filter_response * v_fft

        field_filtered = torch.real(torch.fft.ifftn(v_fft, dim=[2, 3, 4]))
        return field_filtered

    def get_deform_fields(self, deform_flow):
        """Computes the all deformation fields from the deformation flow.
        
        The deformations are organized in a matrix such that the deformation from the deformation that distortes 
        image_i to image_j is given by deform_matrix[j, i]. Another way of viewing the matrix is that the 
        the displacment at time t_j of a particle starting at the identity grid in image_i is given with
        deform_matrix[i, j]. 
        
        Args:
            deform_flow (torch.Tensor or np.ndarray): The deformation flow. Shape ``(N, 3, x, y, z)``
            
        Returns:
            torch.Tensor or np.ndarray: The deformation fields. Shape ``(N, N, 3, x, y, z)``
        """
        # Get the type of the output
        if isinstance(deform_flow, np.ndarray):
            dtype = "np"
        elif isinstance(deform_flow, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("deform_flow must be either numpy array or torch tensor")

        deform_flow = self._convert_to_torch(deform_flow)

        with torch.no_grad():
            deform_matrix = torch.zeros((self.n_img, self.n_img, 3, *self.deform_size), device=self.device)

            deform_field_fwd = self._intergate_flow(deform_flow, "fwd")
            deform_field_bwd = self._intergate_flow(deform_flow, "bwd")

            deform_matrix[utils.get_dig_ind(self.n_img, 1)] = deform_field_fwd
            deform_matrix[utils.get_dig_ind(self.n_img, -1)] = deform_field_bwd

            cum_deform_field_fwd = deform_field_fwd
            cum_deform_field_bwd = deform_field_bwd

            for delta_timestep in range(1, self.n_img - 1):
                (
                    cum_deform_field_fwd,
                    cum_deform_field_bwd,
                ) = self._combine_deform_fields(cum_deform_field_fwd, cum_deform_field_bwd, deform_flow, delta_timestep,
                                                deform_field_fwd, deform_field_bwd)
                deform_matrix[utils.get_dig_ind(self.n_img, delta_timestep + 1)] = cum_deform_field_fwd
                deform_matrix[utils.get_dig_ind(self.n_img, -delta_timestep - 1)] = cum_deform_field_bwd

        if dtype == "np":
            return deform_matrix.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_matrix

    def deform(self, images, deform_field, mode="bilinear", padding_mode="zeros", displacement=True):
        """Transforms the batch of images according to the deformation field.
        
        Args:
            images (torch.Tensor or np.ndarray): The images to deform. Shape ``(N, C, x, y, z)``
            deform_field (torch.Tensor or np.ndarray): The deformation field. Shape ``(N, 3, x, y, z)``
            mode (str, optional): The interpolation mode. Either 'bilinear' or 'nearest'.
            padding_mode (str, optional): The padding mode. Either 'zeros' or 'border'.
            displacement (bool, optional): Whether the deformation field is a displacement field or a deformation field.
            
        Returns:
            torch.Tensor or np.ndarray: The deformed images. Shape ``(N, C, x, y, z)``
        """
        if isinstance(images, np.ndarray):
            dtype = "np"
        elif isinstance(images, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("images must be either numpy array or torch tensor")

        images = self._convert_to_torch(images)
        deform_field = self._convert_to_torch(deform_field)

        if ((mode == self.interpolation_mode) and np.all(np.array(self.img_size) == np.array(images.shape[2:]))):
            with torch.no_grad():
                def_images = self.sp_tr_img(images, deform_field)
        else:
            sp_tr = SpatialTransformer(
                images.shape[2:],
                self.pix_dim,
                mode=mode,
                padding_mode=padding_mode,
            ).to(self.device)
            def_images = sp_tr(images, deform_field, displacement=displacement)

        if dtype == "np":
            return def_images.detach().cpu().numpy()
        elif dtype == "torch":
            return def_images

    def rigid_transform(self, images, rotations, translations):
        grid = self._get_affine_grid(rotations, translations)
        return self.sp_tr_img(images, grid, displacement=False)

    def _convert_to_torch(self, data):
        """
        Converts the data to torch tensor.
        """
        if isinstance(data, np.ndarray):
            return torch.from_numpy(data).to(dtype=torch.float32, device=self.device)
        elif isinstance(data, torch.Tensor):
            return data.to(dtype=torch.float32, device=self.device)
        elif isinstance(data, list):
            return torch.tensor(data, dtype=torch.float32, device=self.device)
        else:
            raise TypeError("Data must be either list, numpy array or torch tensor")


class Registration:
    """Multi Stage Temporal Registration.
    Registrates a series of images. The images are first registrated at a low resolution and then the
    resolution is increased and the images are registrated again. At each resolution the deformation field is
    has a resolution relative to the resampled image resolution set by ``deform_res_scale``.
    
    Accepts same arguments as :class:`StageRegistration` but :attr:`iterations` is replaced by 
    :attr:`stages_iterations`, attr:`deform_res_scale` is replaced by :attr:`stages_deform_scales`, and
    :attr:`img_size` is replaced by :attr:`stages_img_scales`.
    
    Example:
        >>> from muster import Registration
        >>> deform_reg = Registration(
        >>>     stages_iterations=[500, 250, 100],
        >>>     stages_img_scales=[4, 2, 1],
        >>>     stages_deform_scales=[4, 2, 2],
        >>>     img_size=[128, 128, 128],
        >>>     pix_dim=[1, 1, 1],
        >>>     device="cuda:0",
        >>> )
        >>> out = deform_reg.fit(images)
    This example will registrate the image first at a resolution of ``[32, 32, 32]`` with a deformation grid of 
    ``[8, 8, 8]``. Then the resolution is increased to ``[64, 64, 64]`` with a deformation grid of ``[32, 32, 32]``.
    Finally the resolution is increased to ``[128, 128, 128]`` with a deformation grid of ``[64, 64, 64]``.
    
    See :class:`StageRegistration` for more information about the arguments.
    
    Args:
        stages_iterations (list of int): Number of iterations for each stage.
        stages_img_scales (list of int): Image rescaling factors for each stage.
        stages_deform_scales (list of int): Deformation rescaling factors for each stage relative to the image rescaling factor.
        """

    def __init__(self, stages_iterations: list, stages_img_scales: list, stages_deform_scales: list, **kwargs):

        # Make sure the arguments are the same length
        if len(stages_iterations) != len(stages_img_scales) or len(stages_img_scales) != len(stages_deform_scales):
            raise ValueError("stages_iterations, stages_img_scales and stages_deform_scales must have the same length")

        self.stages_iterations = stages_iterations
        self.device = kwargs.get("device", "cpu")
        self.img_sizes = []
        self.deform_scales = stages_deform_scales
        self.img_size = kwargs["img_size"]
        self.pix_dim = kwargs["pix_dim"]
        self.deform_sizes = []
        self.pix_dims = []
        self.deform_pix_dims = []
        self.verbose = kwargs.get("verbose", False)
        self.affine_adjustment = kwargs.get("affine_adjustment", "none")

        for stage in range(len(stages_iterations)):
            img_size = np.array(kwargs["img_size"]) // stages_img_scales[stage]
            self.img_sizes.append(img_size.tolist())
            self.pix_dims.append(np.array(kwargs["pix_dim"]) * stages_img_scales[stage])

            self.deform_sizes.append((np.array(self.img_sizes[stage]) // self.deform_scales[stage]).tolist())
            self.deform_pix_dims.append(np.array(self.pix_dims[stage]) * self.deform_scales[stage])

        # Initialize the deformable registration model of the last stage
        self.kwargs = kwargs
        stage_args = self.kwargs
        stage_args["deform_res_scale"] = self.deform_scales[-1]
        stage_args["num_iterations"] = self.stages_iterations[-1]
        stage_args["img_size"] = self.img_sizes[-1]
        stage_args["pix_dim"] = self.pix_dims[-1]

        self.dr = StageRegistration(**stage_args)

    def fit(self, images, timepoints=None, masks=None):
        """Fits the registration model to the images
        
        Args:
            images (numpy.ndarray or torch.Tensor): The images to registrate. Shape (N, C, x, y, z)
            timepoints (numpy.ndarray or torch.Tensor, optional): The timepoints of the images. Shape (N,)
            masks (numpy.ndarray or torch.Tensor, optional): The masks of the images. Shape (N, C, x, y, z)

        Returns:
            dict: A dictionary containing the deformation field, rotations and translations.
        
        """

        images = self._convert_to_torch(images)

        # Register the images to the current stage resolution
        deform_flow = None
        rotation = None
        translation = None
        affine_matrix = None

        for stage in range(len(self.stages_iterations)):

            # Reinitalize the deformable registration model for the current stage
            stage_args = self.kwargs
            stage_args["deform_res_scale"] = self.deform_scales[stage]
            stage_args["num_iterations"] = self.stages_iterations[stage]
            stage_args["img_size"] = self.img_sizes[stage]
            stage_args["pix_dim"] = self.pix_dims[stage]

            # self.dr = None
            # torch.cuda.empty_cache()
            self.dr = StageRegistration(**stage_args)

            if self.verbose:
                print(
                    f"Stage {stage+1}/{len(self.stages_iterations)} at image resolution {self.img_sizes[stage]}, deformation resolution {self.deform_sizes[stage]}"
                )
            # Resample the images to the current stage resolution
            img_resampled = nn.functional.interpolate(
                images,
                size=self.img_sizes[stage],
                mode="trilinear",
                align_corners=True,
            )
            if masks is not None:
                masks_resampled = nn.functional.interpolate(masks, size=self.img_sizes[stage], mode="nearest")
            else:
                masks_resampled = None

            out = self.dr.fit(
                img_resampled,
                deform_flow,
                timepoints,
                rotation,
                translation,
                affine_matrix,
                masks_resampled,
            )

            deform_flow = torch.tensor(out["deform_flow"], device=self.device)
            if self.affine_adjustment == "rigid":
                rotation = torch.tensor(out["rotations"], device=self.device)
                translation = torch.tensor(out["translations"], device=self.device)
            elif self.affine_adjustment == "affine":
                affine_matrix = torch.tensor(out["affine_matrix"], device=self.device)

            out["deform_flow_not_resampled"] = (deform_flow.detach().cpu().numpy())

            if stage < len(self.stages_iterations) - 1:
                deform_flow = nn.functional.interpolate(
                    deform_flow,
                    size=self.deform_sizes[stage + 1],
                    mode="trilinear",
                    align_corners=True,
                )
        # Add the deform fields
        out["deform_field"] = self.get_deform_fields(deform_flow).detach().cpu().numpy()
        # out["rotations"] = rotation.detach().cpu().numpy()
        # out["translations"] = translation.detach().cpu().numpy()
        #out["affine_matrix"] = affine_matrix.detach().cpu().numpy()
        return out

    def rigid_transform(self, images, rotations, translations):
        """
        Applies the rigid transformation to the images.
        """
        rotations = self._convert_to_torch(rotations)
        translations = self._convert_to_torch(translations)
        return self.dr.rigid_transform(images, rotations, translations)

    def deform(self, images, deform_field, mode="bilinear", padding_mode="zeros", displacement=True):
        """Transforms the batch of images according to the deformation field.
        
        Args:
            images (torch.Tensor or np.ndarray): The images to deform. Shape ``(N, C, x, y, z)``
            deform_field (torch.Tensor or np.ndarray): The deformation field. Shape ``(N, 3, x, y, z)``
            mode (str, optional): The interpolation mode. Either 'bilinear' or 'nearest'.
            padding_mode (str, optional): The padding mode. Either 'zeros' or 'border'.
            displacement (bool, optional): Whether the deformation field is a displacement field or a deformation field.
            
        Returns:
            torch.Tensor or np.ndarray: The deformed images. Shape ``(N, C, x, y, z)``
        """
        return self.dr.deform(images, deform_field, mode, padding_mode, displacement)

    def get_deform_fields(self, deform_flow):
        """Computes the all deformation fields from the deformation flow.
        
        The deformations are organized in a matrix such that the deformation from the deformation that distortes 
        image_i to image_j is given by deform_matrix[j, i]. Another way of viewing the matrix is that the 
        the displacment at time t_j of a particle starting at the identity grid in image_i is given with
        deform_matrix[i, j]. 
        
        Args:
            deform_flow (torch.Tensor or np.ndarray): The deformation flow. Shape ``(N, 3, x, y, z)``
            
        Returns:
            torch.Tensor or np.ndarray: The deformation fields. Shape ``(N, N, 3, x, y, z)``
        """

        if isinstance(deform_flow, np.ndarray):
            dtype = "np"
        elif isinstance(deform_flow, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("deform_field must be either numpy array or torch tensor")

        deform_flow = self._convert_to_torch(deform_flow)
        if self.deform_scales[-1] != 1:
            with torch.no_grad():
                deform_matrix = self.dr.get_deform_fields(deform_flow)
                n_img = deform_matrix.shape[0]
                deform_matrix = nn.functional.interpolate(
                    deform_matrix.reshape((-1, 3, *self.deform_sizes[-1])),
                    size=self.img_size,
                    mode="trilinear",
                    align_corners=True,
                )
                deform_matrix = deform_matrix.reshape((
                    n_img,
                    n_img,
                    3,
                    *self.img_size,
                ))
        else:
            with torch.no_grad():
                deform_matrix = self.dr.get_deform_fields(deform_flow)

        if dtype == "np":
            return deform_matrix.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_matrix

    def _convert_to_torch(self, data):
        """Converts the data to torch tensor.
        
        Args:
            data (numpy.ndarray or torch.Tensor): The data to convert.
            
        Returns:
            torch.Tensor: The data converted to torch tensor.
        """
        if isinstance(data, np.ndarray):
            return torch.from_numpy(data).to(dtype=torch.float32, device=self.device)
        elif isinstance(data, torch.Tensor):
            return data.to(dtype=torch.float32, device=self.device)
        else:
            raise TypeError("Data must be either numpy array or torch tensor")

    def get_identity_grid(self):
        """Returns the identity grid.
        """

        return self.dr.sp_tr_img.grid.detach().cpu().numpy()


class SpatialTransformer(nn.Module):
    r""" Spatial Transformer for performing grid pull operations on spaces.
    Based on https://github.com/voxelmorph/voxelmorph. The spatial transformer uses a deformation field to deform
    the input space. The deformation field is given in mm/step. 
    
    Args:
        size (tuple): the target size of the output tensor. shape ``(x, y, z)``
        pix_dim (tuple): the pixel spacing (mm/px) of the output tensor ``(dx, dy, dz)``
        mode ('bilinear' or 'nearest'): interpolation mode
        padding_mode ('zeros' or 'border'): padding mode
    """

    def __init__(self, size, pix_dim=(1, 1, 1), mode="bilinear", padding_mode="zeros"):
        super().__init__()

        self.mode = mode
        self.padding_mode = padding_mode
        self.pix_dim = pix_dim
        self.size = size
        self.size_mm = [(s - 1) * p for s, p in zip(size, pix_dim)]

        # Create sampling grid
        vectors = [torch.linspace(0, self.size_mm[i], steps=self.size[i]) for i in range(len(self.size))]
        grids = torch.meshgrid(vectors, indexing='ij')
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.float)
        self.register_buffer("grid", grid)

    def forward(self, field, transformation_field, displacement=True):
        r"""Deforms the field image with the transformation field with units in mm/step.

            Args:
                field: the source space to be transformed
                transformation_field: the transformation field to be applied to the source image, with units in mm/step
                displacement: whether the transformation field is a displacement field or a deformation field
                
            Returns:
                torch.Tensor: the transformed source image
            """

        if displacement:
            new_locs = self.grid + transformation_field
        else:
            new_locs = transformation_field.clone()

        # need to normalize grid values to [-1, 1] for resampler
        for axis_idx in range(len(self.size_mm)):
            new_locs[:, axis_idx, ...] = 2 * (new_locs[:, axis_idx, ...] / (self.size_mm[axis_idx]) - 0.5)

        if len(self.size_mm) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(self.size_mm) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(
            field,
            new_locs,
            align_corners=True,
            mode=self.mode,
            padding_mode=self.padding_mode,
        )
