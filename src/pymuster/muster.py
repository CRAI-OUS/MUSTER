"""Multi Session Temporal Registration.

This module is the core of Multi Session Temporal Registration (MUSTER).

Classes:
    StageRegistration: Register a longitudinal series of images using a series of deformation fields using a single resolution.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np

from alive_progress import alive_bar

from .core import field_calculus
from .core import losses
from .core import utils

import warnings


class StageRegistration:
    """Register a longitudinal series of images using a series of deformation fields using a single resolution.

    The images must be in the same space and must be pre-registered using an rigid or affine transformation.

    The deformation between two consectutive timepoints is represented by a deformation field. These fields are 
    generated by integrating a vector field, also known as the deformation flow. Various methods for 
    this integration are supported:
        * ``integration_method="ss"``: Use scaling and squaring to integrate stationary vector fields. (Default)
        * ``integration_method=euler``: Use euler integration for stationary vector fields.
        * ``integration_method=rk4``: Use Runge-Kutta 4 integration for stationary vector field.
        * ``integration_method=euler_time``: Use euler integration for time varying vector fields.
        * ``integration_method=rk4_time``: Use Runge-Kutta 4 integration for time varying vector fields.

    For non-consecutive timepoints, the deformation field can be composed in two ways:
        * ``field_composition_method="interpolate"``: Use interpolation to compose the intermediate deformation 
            fields. (Default)
        * ``field_composition_method="flow_continuation"``: Continuate the deformation field by integrating the 
            intermediate deformation flows.

    Supported image similarity metrics include:
        * ``img_similarity_metric="NCC"``: Normalized local cross correlation between the two images as loss. 
            The local neighborhood is a cube of size with side length ``img_similarity_spatial_size``.
        * ``img_similarity_metric="L2"``: L2 norm of the difference between the two images as loss. 
        * ``img_similarity_metric="NCCS"``: Use a sobel filter to compute the gradient of the two images, and use the 
            normalized local cross correlation between the two gradients as loss. The local window
            is a cube of size with side length ``3``.
        * ``img_similarity_metric="WNCC"``: Normalized local cross correlation between the two images as loss where each 
            local neighborhood is weighted by the cross standard deviation of the two images.
            The local neighborhood is a cube of size with side length ``img_similarity_spatial_size``.
        * ``img_similarity_metric="GaussNCC"``: Normalized local cross correlation between the two images as loss where
            each local neighborhood is weighted by a gaussian filter. The standard deviation of the gaussian filter is 
            ``img_similarity_spatial_size``.
        * ``img_similarity_metric="Fourier"``: Use the Fourier tranform to compute a filtered gradient of each images,
            and compute the global normalized cross correlation between the two filtered gradients as loss. 
            The standard deviation of the gaussian filter is ``img_similarity_spatial_size``.


    Args:
        image_size (tuple): The size of the input images in the format ``(x, y, z)``.
        pix_dim (tuple): The pixel dimensions of the input images in the format ``(dx, dy, dz)``. Default is ``(1, 1, 1)``.
        deform_res_scale (int): The resolution scale of the deformation field. The deformation field will have a 
            resolution of image_size/deform_res_scale. Default is ``1``.
        device (str): The device to use for computation. ('cpu' or 'cuda:n' or torch.device object). Default is ``"cpu"``.
        num_iterations (int): Number of optimization iterations. Default is ``100``.
        spatial_smoothness_penalty (float): Weight for spatial smoothness in loss function. Default is ``1.0``.
        temporal_smoothness_penalty (float): Weight for temporal smoothness in loss function. Default is ``1.0``.
        invertability_penalty (float): Weight for invertibility penalty in loss function. Default is ``1.0``.
        l2_penalty (float): Weight for L2 penalty on the deformation flow. Default is ``0.0``.
        smoothing_sigma (float): Sigma for Gaussian smoothing of deformation flows. Default is ``1.0``.
        mode (str): The mode for interpolation of the images. ('nearest', 'bilinear', or 'bicubic'). Default is ``"bilinear"``.
        integration_steps (int): The number of integration steps for integrating deformation flow. Default is ``7``.
        integration_method (str): The method for integrating deformation field.
            ('ss', 'euler', 'rk4', 'euler_time', or 'rk4'). Default is ``"ss"``.
        field_composition_method (str): The method for composing deformation fields.
            ('interpolate' or 'flow_continuation'). Default is ``"interpolate"``.
        affine_adjustment (str): The method for adjusting the affine transformation. ('none', 'rigid', or 'affine'). Default is ``"none"``.
        learning_rate (float): Learning rate for optimizer. Default is ``1e-3``.
        betas (tuple): The beta coefficients for Adam optimizer. Default is ``(0.9, 0.999)``.
        tol (float): Tolerance for optimization convergence. Default is ``1e-4``.
        img_similarity_metric (str): The image similarity metric ('NCC', 'L2', 'NCCS', 'WNCC', 'GaussNCC', or 'Fourier').
            Default is ``"NCC"``.
        img_similarity_spatial_size (int/float): Size or standard deviation of the local neighborhood for the image 
            similarity metric. Default is ``3``.
        verbose (bool): Flag for printing progress during optimization. Default is ``True``.
    """

    def __init__(
        self,
        image_size: tuple,
        pix_dim: tuple = (1, 1, 1),
        deform_res_scale: int = 1,
        device: str = "cpu",
        num_iterations: int = 100,
        spatial_smoothness_penalty: float = 1.0,
        spatial_jac_smoothness_penalty: float = 1.0,
        spatial_jac_tissue_smoothness_penalty: float = 0.0,
        temporal_smoothness_penalty: float = 1.0,
        invertability_penalty: float = 1.0,
        l2_penalty: float = 0.0,
        smoothing_sigma: float = 1.0,
        mode: str = "bilinear",
        integration_steps: int = 7,
        integration_method: str = "ss",
        field_composition_method: str = "interpolate",
        affine_adjustment: str = "none",
        learning_rate: float = 1e-3,
        betas: tuple = (0.9, 0.999),
        tol: float = 1e-4,
        img_similarity_metric: str = "NCC",
        img_similarity_scale_invariant: bool = True,
        img_similarity_spatial_size: int or float = 3,
        verbose: bool = True,
    ):
        # Valid values for integration_method
        valid_integration_methods = ["ss", "euler", "rk4", "euler_time", "rk4_time"]
        if integration_method not in valid_integration_methods:
            raise ValueError(f"integration_method must be one of {valid_integration_methods}")

        # Valid values for field_composition_method
        valid_field_composition_methods = ["interpolate", "flow_continuation"]
        if field_composition_method not in valid_field_composition_methods:
            raise ValueError(f"field_composition_method must be one of {valid_field_composition_methods}")

        # flow_continuation can not be combined with ss
        if field_composition_method == "flow_continuation" and integration_method == "ss":
            raise ValueError("flow_continuation can not be combined with ss")

        # Valid values for img_similarity_metric
        valid_img_similarity_metrics = ["NCC", "L2", "NCCS", "Fourier", "WNCC", "GaussNCC", "VELLN", "VELLNGAUSS"]
        if img_similarity_metric not in valid_img_similarity_metrics:
            raise ValueError(f"img_similarity_metric must be one of {valid_img_similarity_metrics}")

        # Valid values for mode
        valid_modes = ["nearest", "bilinear", "bicubic"]
        if mode not in valid_modes:
            raise ValueError(f"mode must be one of {valid_modes}")

        # Check the dimensions of the image
        if len(image_size) != 3:
            raise ValueError("image_size must be a tuple of length 3")

        # Check the dimensions of the pixel dimensions
        if len(pix_dim) != 3:
            raise ValueError("pix_dim must be a tuple of length 3")

        self.device = torch.device(device)

        self.num_iterations = num_iterations
        self.spatial_smoothness_penalty = spatial_smoothness_penalty
        self.spatial_jac_smoothness_penalty = spatial_jac_smoothness_penalty
        self.spatial_jac_tissue_smoothness_penalty = spatial_jac_tissue_smoothness_penalty
        self.temporal_smoothness_penalty = temporal_smoothness_penalty
        self.l2_penalty = l2_penalty
        self.smoothing_sigma = smoothing_sigma
        self.integration_steps = integration_steps
        self.integration_method = integration_method
        self.field_composition_method = field_composition_method

        self.affine_adjustment = affine_adjustment
        self.betas = betas
        self.learning_rate = learning_rate
        self.image_size = image_size
        self.deform_res_scale = deform_res_scale
        self.verbose = verbose
        self.tol = tol
        self.img_similarity_metric = img_similarity_metric
        self.pix_dim = pix_dim
        self.invertability_penalty = invertability_penalty
        self.img_similarity_spatial_size = img_similarity_spatial_size

        self.deform_size = [int(s // self.deform_res_scale) for s in self.image_size]
        self.deform_pix_dim = [s * self.deform_res_scale for s in self.pix_dim]
        self.mode = mode

        self.sp_tr = SpatialTransformer(
            self.deform_size,
            self.deform_pix_dim,
            mode=mode,
            padding_mode="border",
        ).to(self.device)

        if self.img_similarity_metric == "MSE" or self.img_similarity_metric == 'L2':
            self.img_sim_metric_fnc = losses.MSE()
        elif self.img_similarity_metric == "NCC":
            self.img_sim_metric_fnc = losses.NCC(
                self.img_similarity_spatial_size, scale_invariant=img_similarity_scale_invariant)
        elif self.img_similarity_metric == "GaussNCC":
            if self.img_similarity_spatial_size < self.pix_dim[0] + 1:
                warnings.warn(
                    f"img_similarity_spatial_size ({self.img_similarity_spatial_size}) is smaller than the pixel size ({self.pix_dim[0] + 1})."
                    " This may cause the gaussian filter to return NaNs."
                    " Setting img_similarity_spatial_size to the pixel size + 1.")
                self.img_similarity_spatial_size = self.pix_dim[0] + 1
            self.img_sim_metric_fnc = losses.GaussNCC(
                self.img_similarity_spatial_size,
                self.image_size,
                self.pix_dim,
                scale_invariant=img_similarity_scale_invariant)
        elif self.img_similarity_metric == "NCCS":
            self.img_sim_metric_fnc = losses.NCCS(self.img_similarity_spatial_size)
        elif self.img_similarity_metric == "VELLN":
            self.img_sim_metric_fnc = losses.VELLN(self.img_similarity_spatial_size, kernal_type='window')
        elif self.img_similarity_metric == "VELLNGAUSS":
            self.img_sim_metric_fnc = losses.VELLN(
                self.img_similarity_spatial_size,
                kernal_type='gaussian',
                pix_dim=self.pix_dim,
                image_size=self.image_size)

        self.sp_tr_img = SpatialTransformer(self.image_size, pix_dim, mode=mode, padding_mode="border").to(self.device)

    def fit(
        self,
        images,
        inital_deform_flow=None,
        inital_sigmas=None,
        timepoints=None,
        initial_rotations=None,
        initial_translations=None,
        initial_affine=None,
        masks=None,
    ):
        """Fits the deformation flow to the images.

        Args:
            images (torch.Tensor or np.ndarray): shape ``(N, channels, x, y, z)``
            inital_deform_flow (torch.Tensor or np.ndarray): shape ``(N-1, 3, x, y, z)``
            timepoints (torch.Tensor or np.ndarray): shape ``(N,)``
                The timepoints of the images. Used to adjust the temporal
                penalties. May be None, in which case the timepoints are
                assumed to be equally spaced.

        Returns:
            out (dict): Dictionary containing the deformation flow and optionally the rotations and translations.
        """

        self.n_img = images.shape[0]  # Number of images
        if self.integration_method in ['ss', 'euler', 'rk4']:
            self.n_flow = self.n_img - 1  # Number of deformations flow fields
        else:
            self.n_flow = self.n_img

        # Set device
        device = self.device

        images = self._convert_to_torch(images)

        if masks is None:
            masks = torch.ones_like(images)

        # The time difference between the images
        if timepoints is None:
            delta_times = torch.ones((self.n_img - 1, 1, 1, 1, 1), device=device)
        else:
            timepoints = self._convert_to_torch(timepoints)
            delta_times = timepoints[1:] - timepoints[:-1]
            delta_times = delta_times[:, None, None, None, None]

        # The deformation flow
        if inital_deform_flow is None:
            deform_flow = nn.parameter.Parameter(torch.zeros((self.n_flow, 3, *self.deform_size), device=device))
        else:
            inital_deform_flow = self._convert_to_torch(inital_deform_flow)
            deform_flow = nn.parameter.Parameter(inital_deform_flow.clone().to(device))

        param_groups = [{"params": [deform_flow], "lr": self.learning_rate}]

        # Parameters for rigid ajustment of the images
        if self.affine_adjustment == "rigid":
            if initial_rotations is None:
                rotation_params = nn.Parameter(torch.zeros((self.n_img - 1, 3), device=device))
            else:
                initial_rotations = self._convert_to_torch(initial_rotations[1:])
                rotation_params = nn.Parameter(initial_rotations.clone().to(device))

            if initial_translations is None:
                translation_params = nn.Parameter(torch.zeros((self.n_img - 1, 3), device=device))
            else:
                initial_translations = self._convert_to_torch(initial_translations[1:])
                translation_params = nn.Parameter(initial_translations.clone().to(device))

            param_groups.append({
                "params": [rotation_params, translation_params],
                "lr": self.learning_rate * 0.0001,
            })
        elif self.affine_adjustment == "affine":
            if initial_affine is None:
                affine_params = nn.Parameter((torch.eye(4, device=device)[:3, :]).repeat(self.n_img - 1, 1, 1))
            else:
                initial_affine = self._convert_to_torch(initial_affine)
                affine_params = nn.Parameter(initial_affine.clone().to(device))

            param_groups.append({
                "params": [affine_params],
                "lr": self.learning_rate * 0.1,
            })

        if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
            log_sigmas = nn.parameter.Parameter(torch.ones((self.n_img, 1, 1, 1, 1), device=device) * (-1.0))
            param_groups.append({"params": [log_sigmas], "lr": self.learning_rate * 10})

        optimizer = torch.optim.Adam(param_groups, lr=self.learning_rate, betas=self.betas)
        scaler = torch.cuda.amp.GradScaler()

        # Cosine annealing with linear warmup
        warmup_steps = int(self.num_iterations * 0.2)
        scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1 / (warmup_steps) * (step + 1))
        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=int(self.num_iterations - warmup_steps))
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer, schedulers=[scheduler1, scheduler2], milestones=[warmup_steps])

        # Object for keeping track of convergence of the optimization process
        running_mean_var = utils.RunningMeanVar()

        use_autocast = False  #torch.cuda.is_available() and (str(device) != "cpu")

        with alive_bar(
                self.num_iterations,
                force_tty=True,
                max_cols=130,
                dual_line=True,
                title_length=50,
                elapsed="({elapsed})",
                stats="(eta: {eta})",
                disable=not self.verbose,
        ) as bar:
            for iteration_idx in range(self.num_iterations):
                optimizer.zero_grad()

                with torch.cuda.amp.autocast(use_autocast):
                    if self.affine_adjustment == "rigid":
                        # Add the identity transformation to first timepoint
                        rots = torch.cat((torch.zeros((1, 3), device=device), rotation_params), dim=0)
                        trans = torch.cat((torch.zeros((1, 3), device=device), translation_params), dim=0)
                        tran_grid = self._get_affine_grid(rots, trans)

                    else:
                        rots = torch.zeros((self.n_img, 3), device=device)
                        trans = torch.zeros((self.n_img, 3), device=device)
                        tran_grid = self._get_affine_grid(rots, trans)

                    batch = images
                    loss = 0
                    image_loss = 0
                    inv_loss = 0

                    # The cumulative deformation fields
                    cum_deform_field_fwd = self._intergate_flow(deform_flow, dir="fwd")  # mm/step
                    cum_deform_field_bwd = self._intergate_flow(deform_flow, dir="bwd")  # mm/step

                    # The consecutive deformation fields
                    deform_field_fwd = cum_deform_field_fwd
                    deform_field_bwd = cum_deform_field_bwd

                    # Regularize the deformation field
                    if self.integration_method == "ss":
                        loss += self._spatial_continuity_loss(deform_flow / torch.sqrt(delta_times))
                    else:
                        loss += self._spatial_continuity_loss(deform_flow)

                    loss += self.l2_penalty * torch.mean((deform_flow / torch.sqrt(delta_times))**2)

                    if self.n_img > 2:
                        loss += (
                            self.temporal_smoothness_penalty * self._temperal_continuity_loss(deform_flow, delta_times))

                    # Loop over the seperation between the images starting with consecutive images and ending with the
                    # first and last image
                    for delta_timestep in range(self.n_img - 1):
                        if delta_timestep > 0:
                            (cum_deform_field_fwd, cum_deform_field_bwd) = self._commute_deform_fields(
                                cum_deform_field_fwd,
                                cum_deform_field_bwd,
                                deform_flow,
                                delta_timestep,
                                deform_field_fwd,
                                deform_field_bwd,
                            )

                        # Interpolate the cum_deform_fields to the resolution of the images
                        if np.any(np.array(self.image_size) != np.array(self.deform_size)):
                            cum_deform_field_fwd_interp = nn.functional.interpolate(
                                cum_deform_field_fwd,
                                size=self.image_size,
                                mode="trilinear",
                                align_corners=True,
                            )
                            cum_deform_field_bwd_interp = nn.functional.interpolate(
                                cum_deform_field_bwd,
                                size=self.image_size,
                                mode="trilinear",
                                align_corners=True,
                            )
                        else:
                            cum_deform_field_fwd_interp = cum_deform_field_fwd
                            cum_deform_field_bwd_interp = cum_deform_field_bwd

                        sample = self.sp_tr_img(
                            batch[:-delta_timestep - 1, ...],
                            cum_deform_field_fwd_interp + tran_grid[:-delta_timestep - 1, ...],
                            displacement=False)

                        if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
                            image_loss += self.img_sim_metric_fnc.loss(
                                batch[delta_timestep + 1:, ...],
                                sample,
                                mask=masks[delta_timestep + 1:, ...],
                                sigma_true=log_sigmas[delta_timestep + 1:, ...],
                                sigma_pred=log_sigmas[:-delta_timestep - 1, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        else:
                            image_loss += self.img_sim_metric_fnc.loss(
                                batch[delta_timestep + 1:, ...],
                                sample,
                                mask=masks[delta_timestep + 1:, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        sample = self.sp_tr_img(
                            batch[delta_timestep + 1:, ...],
                            cum_deform_field_bwd_interp + tran_grid[delta_timestep + 1:, ...],
                            displacement=False)

                        if self.img_similarity_metric == "VELLN" or self.img_similarity_metric == "VELLNGAUSS":
                            image_loss += self.img_sim_metric_fnc.loss(
                                batch[:-delta_timestep - 1, ...],
                                sample,
                                mask=masks[:-delta_timestep - 1, ...],
                                sigma_true=log_sigmas[:-delta_timestep - 1, ...],
                                sigma_pred=log_sigmas[delta_timestep + 1:, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        else:
                            image_loss += self.img_sim_metric_fnc.loss(
                                batch[:-delta_timestep - 1, ...],
                                sample,
                                mask=masks[:-delta_timestep - 1, ...],
                            ) * (
                                self.n_img - 1 - delta_timestep)

                        if self.invertability_penalty > 0:
                            inv_loss += self.invertability_penalty * self._invertability_loss(
                                cum_deform_field_fwd, cum_deform_field_bwd) * (
                                    self.n_img - 1 - delta_timestep)

                    # Normalize with respect to the number of images
                    image_loss /= (self.n_img - 1) * self.n_img
                    inv_loss /= (self.n_img - 1) * self.n_img

                    # Combine the losses
                    loss += image_loss + inv_loss

                if use_autocast:
                    # Propagate the loss and update the parameters
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    optimizer.step()

                # Ignore the warning from the scheduler
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    scheduler.step()

                bar()
                bar.title(f"\tLoss: {loss.item():.4e}, Image Loss: {image_loss.item():.4e}")
                bar.title_length = 200

                running_mean_var.add(loss.item())

                if self.verbose:
                    if iteration_idx == self.num_iterations - 1:
                        bar.title(f"\tMax iterations reached")

                # if (iteration_idx > 20 and np.sqrt(running_mean_var.var()) / running_mean_var.mean() < self.tol):
                #     bar.title(f"\tConvergence reached")
                #     break

        optimizer.zero_grad()

        return_dict = {
            "deform_flow": deform_flow.detach().cpu().numpy(),
        }
        if self.affine_adjustment == "rigid":
            return_dict["rotations"] = rotation_params.detach().cpu().numpy()
            return_dict["translations"] = translation_params.detach().cpu().numpy()
        elif self.affine_adjustment == "affine":
            return_dict["affine_matrix"] = affine_params.detach().cpu().numpy()

        return return_dict

    def _get_homogeneous_transformation_matrix(self, rotation, translation):
        """Computes the affine matrix from rotation and translation
        """
        Rx = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Rx[:, [1, 2], [1, 2]] = torch.cos(rotation[:, 0])[:, None]
        Rx[:, 1, 2] = -torch.sin(rotation[:, 0])
        Rx[:, 2, 1] = torch.sin(rotation[:, 0])

        Ry = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Ry[:, [0, 2], [0, 2]] = torch.cos(rotation[:, 1])[:, None]
        Ry[:, 0, 2] = torch.sin(rotation[:, 1])
        Ry[:, 2, 0] = -torch.sin(rotation[:, 1])

        Rz = torch.eye(3, device=self.device).repeat(rotation.shape[0], 1, 1)
        Rz[:, [0, 1], [0, 1]] = torch.cos(rotation[:, 2])[:, None]
        Rz[:, 0, 1] = -torch.sin(rotation[:, 2])
        Rz[:, 1, 0] = torch.sin(rotation[:, 2])

        R = torch.matmul(Rz, torch.matmul(Ry, Rx))
        matrix = torch.cat([R, translation[:, :, None]], dim=2)
        return matrix

    def _get_affine_grid(self, rotation=None, translation=None, matrix=None, type='img'):
        """
        Computes the affine grid from rotation and translation
        """

        if matrix is None:
            matrix = self._get_homogeneous_transformation_matrix(rotation, translation)

        if type == 'img':
            grid = self.sp_tr_img.grid
        else:
            grid = self.sp_tr.grid

        # Grid is in the shape (N, 3, H, W, D)
        # Matrix is in the shape (N, 3, 4)
        grid = (torch.einsum("nij, njhwd -> nihwd", matrix[:, :3, :3], grid) + matrix[:, :3, 3, None, None, None])
        return grid

    def _invertability_loss(self, deform_field_fwd, deform_field_bwd):
        method = "ICON"  # 'ICON' or 'gradICON'
        if method == "Gamma":
            loss = 0
            for deform_field in [deform_field_fwd, deform_field_bwd]:
                J = field_calculus.jacobian(deform_field, pix_dim=self.deform_pix_dim)
                margin = 0
                loss = 0
                loss += torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**
                                   2) * self.spatial_smoothness_penalty

                det_J = field_calculus.determinant(J +
                                                   torch.eye(3, device=self.device)[None, :, :, None, None, None])[:,
                                                                                                                   None,
                                                                                                                   ...]
                #det_J = field_calculus.determinant(J)[:, None, ...]

                sigma_det_J = 1.0
                theta = 0.5 * (np.sqrt(1 + 4 * sigma_det_J**2) - 1)
                k = sigma_det_J**2 / theta**2
                loss += torch.mean(-(k - 1) * torch.log(det_J) + det_J / theta - 1 / theta)
            return loss
        else:
            grid = self.sp_tr.grid

            deform_pix_dim = torch.tensor(self.deform_pix_dim, device=self.device)[None, :, None, None, None]

            fwd_loss = (self.sp_tr(deform_field_bwd + grid, deform_field_fwd) - grid) / deform_pix_dim
            bwd_loss = (self.sp_tr(deform_field_fwd + grid, deform_field_bwd) - grid) / deform_pix_dim

            if method == "ICON":
                return torch.mean(fwd_loss**2 + bwd_loss**2)
            elif method == "gradICON":
                J = field_calculus.jacobian(torch.cat([fwd_loss, bwd_loss], dim=0), self.deform_pix_dim)
                margin = 2
                J = J[:, :, :, margin:-margin, margin:-margin, margin:-margin]
                return torch.mean((J)**2)

    def _spatial_continuity_loss(self, deform_flow):
        # J = field_calculus.jacobian(deform_flow, pix_dim=self.deform_pix_dim)
        # margin = 0
        # loss = 0
        # loss += torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**2) * 1.0
        # det_J = field_calculus.determinant(J + torch.eye(3, device=self.device)[None, :, :, None, None, None])[:, None,
        #                                                                                                        ...]
        # J_det_J = field_calculus.jacobian(det_J, pix_dim=self.deform_pix_dim)
        # J_det_J = J_det_J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]
        # loss += torch.mean(J_det_J**2) * 1.0
        # return loss

        J = field_calculus.jacobian(deform_flow, pix_dim=self.deform_pix_dim)
        margin = 0
        loss = 0
        # Normal distribution prior over the Jacobian
        loss += torch.mean(J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]**
                           2) * self.spatial_smoothness_penalty
        det_J = field_calculus.determinant(J + torch.eye(3, device=self.device)[None, :, :, None, None, None])[:, None,
                                                                                                               ...]
        # # Gamma distribution prior over the determinant of the Jacobian
        # sigma_det_J = 0.02
        # theta = 0.5 * (np.sqrt(1 + 4 * sigma_det_J**2) - 1)
        # k = sigma_det_J**2 / theta**2
        # loss += torch.mean(-(k - 1) * torch.log(det_J) + det_J / theta - 1 / theta) * 1

        J_det_J = field_calculus.jacobian(det_J, pix_dim=self.deform_pix_dim)
        J_det_J = J_det_J[:, :, :, margin:-margin - 1, margin:-margin - 1, margin:-margin - 1]

        loss += torch.mean(J_det_J**2 * self.spatial_jac_smoothness_penalty)

        return loss

    def _temperal_continuity_loss(self, deform_flow, delta_times):
        if (self.integration_method == "ss") or (self.integration_method == "euler") or (self.integration_method
                                                                                         == "rk4"):
            return torch.mean(
                ((deform_flow[1:, ...] / delta_times[1:, ...] - deform_flow[:-1, ...] / delta_times[:-1, ...])**2))
        else:
            return torch.mean(((deform_flow[1:, ...] - deform_flow[:-1, ...]) / delta_times)**2)

    def _intergate_flow(self, deform_flow, dir, target_grid=None):
        """
        Integrates a vector field via scaling and squaring.
        Args:
            deform_flow: The vector field to integrate. Shape (N, 3, x, y, z)
            dir: The direction to integrate in. ('fwd' or 'bwd')
            target_grid: The grid to start the integration from. If None, the grid will be set to the identity transformation.
        """
        super().__init__()
        nsteps = self.integration_steps
        method = self.integration_method

        if target_grid is None:
            if method in ["ss", "euler", "rk4"]:
                target_grid = (self.sp_tr.grid.detach().clone().repeat(deform_flow.shape[0], 1, 1, 1, 1))
            else:
                target_grid = (self.sp_tr.grid.detach().clone().repeat(deform_flow.shape[0] - 1, 1, 1, 1, 1))

        if self.smoothing_sigma > 0:
            deform_flow = self._gaussian_smooth(deform_flow, self.smoothing_sigma)

        if method == "ss":
            # Integrate a constant vector field with scaling and squaring
            scale = 1.0 / (2**nsteps)
            if dir in ["backward", "bwd"]:
                scale = -scale
            vec = deform_flow * scale
            for _ in range(nsteps):
                vec = vec + self.sp_tr(vec, vec)
            return vec

        elif method == "euler":
            # Integrate a constant vector field with euler integration
            vec = torch.zeros_like(deform_flow)
            dt = 1.0 / nsteps
            if dir in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                vec = vec + self.sp_tr(deform_flow * dt, vec + target_grid, displacement=False)
            return vec
        elif method == "euler_time":
            # Integrate a constant vector field with euler integration
            vec = torch.zeros(
                (deform_flow.shape[0] - 1, 3, *self.deform_size),
                device=self.device,
            )
            dt = 1.0 / nsteps
            if dir in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                if dir in ["backward", "bwd"]:
                    flow = (1 - i / nsteps) * deform_flow[1:, ...] + (i / nsteps) * deform_flow[:-1, ...]
                else:
                    flow = (1 - i / nsteps) * deform_flow[:-1, ...] + (i / nsteps) * deform_flow[1:, ...]

                vec = vec + self.sp_tr(flow * dt, vec + target_grid, displacement=False)
            return vec

        elif method == "rk4":
            vec = torch.zeros_like(deform_flow)
            dt = 1.0 / nsteps
            if dir in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                k1 = self.sp_tr(deform_flow * dt, vec + target_grid, displacement=False)
                k2 = self.sp_tr(
                    deform_flow * dt,
                    vec + target_grid + k1 * 0.5,
                    displacement=False,
                )
                k3 = self.sp_tr(
                    deform_flow * dt,
                    vec + target_grid + k2 * 0.5,
                    displacement=False,
                )
                k4 = self.sp_tr(deform_flow * dt, vec + target_grid + k3, displacement=False)
                vec = vec + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0
            return vec
        elif method == "rk4_time":
            vec = torch.zeros(
                (deform_flow.shape[0] - 1, 3, *self.deform_size),
                device=self.device,
            )
            dt = 1.0 / nsteps
            if dir in ["backward", "bwd"]:
                dt = -dt
            for i in range(nsteps):
                if dir in ["backward", "bwd"]:
                    flow = (1 - i / nsteps) * deform_flow[1:, ...] + (i / nsteps) * deform_flow[:-1, ...]
                else:
                    flow = (1 - i / nsteps) * deform_flow[:-1, ...] + (i / nsteps) * deform_flow[1:, ...]
                k1 = self.sp_tr(flow * dt, vec + target_grid, displacement=False)
                k2 = self.sp_tr(flow * dt, vec + target_grid + k1 * 0.5, displacement=False)
                k3 = self.sp_tr(flow * dt, vec + target_grid + k2 * 0.5, displacement=False)
                k4 = self.sp_tr(flow * dt, vec + target_grid + k3, displacement=False)
                vec = vec + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0
            return vec
        elif method == "trapz":
            # A secret method that also works
            # Find the vector field that transforms the midpoint grid to the target grid
            k = 0.5
            midpoint_grid = target_grid
            landing_grid = target_grid
            if dir in ["backward", "bwd"]:
                dir = -1
            elif dir in ["forward", "fwd"]:
                dir = 1

            for i in range(nsteps):
                midpoint_grid = midpoint_grid - \
                    k * (landing_grid - target_grid)
                # The boundary condition is that the deformation field continue outside the grid,
                landing_grid = (self.sp_tr(-deform_flow * 0.5 * dir, midpoint_grid, displacement=False) + midpoint_grid)

            return dir * self.sp_tr(
                deform_flow,
                midpoint_grid - k * (landing_grid - target_grid),
                displacement=False,
            )

    def _commute_deform_fields(
        self,
        cum_deform_field_fwd,
        cum_deform_field_bwd,
        deform_flow,
        delta_timestep,
        deform_field_fwd,
        deform_field_bwd,
    ):
        """Adds the integral of the deformation flow to the cumulative deformation fields.
        
            If ``field_composition_method`` is set to ``interpolate``, the composition is done by interpolation of 
            the consecutive deformation fields with the cumulative deformation fields.
            If ``field_composition_method`` is set to ``flow_continuation``, the composition is done by integrating 
            the deformation flow from the enpoint of the cumulative deformation fields.
            
            ``delta_timestep`` gives the number of times the cumulative deformation fields has been integrated.
            
            Args:
                cum_deform_field_fwd (torch.Tensor): Cumulative forward deformation field.
                cum_deform_field_bwd (torch.Tensor): Cumulative backward deformation field.
                deform_flow (torch.Tensor): Deformation flow.
                delta_timestep (int): The number of timesteps the cumulative deformation fields has been integrated.
                deform_field_fwd (torch.Tensor): Consecutive forward deformation field.
                deform_field_bwd (torch.Tensor): Consecutive backward deformation field.

            Returns:
                tuple: A tuple containing:

                    - **cum_deform_field_fwd** (torch.Tensor): Cumulative forward deformation field.
                    - **cum_deform_field_bwd** (torch.Tensor): Cumulative backward deformation field.
            """
        Nd = deform_field_fwd.shape[0]

        method = self.field_composition_method

        fwd_grid = cum_deform_field_fwd[:-1, ...] + self.sp_tr.grid
        bwd_grid = cum_deform_field_bwd[1:, ...] + self.sp_tr.grid

        if method == "interpolate":
            cum_deform_field_fwd = cum_deform_field_fwd[:-1, ...] + self.sp_tr(
                deform_field_fwd[delta_timestep:, ...],
                fwd_grid,
                displacement=False,
            )
            cum_deform_field_bwd = cum_deform_field_bwd[1:, ...] + self.sp_tr(
                deform_field_bwd[:Nd - delta_timestep, ...],
                bwd_grid,
                displacement=False,
            )

            # cum_deform_field_fwd = cum_deform_field_fwd[:-1, ...] + self.sp_tr(
            #     deform_field_fwd[delta_timestep:, ...],
            #     fwd_grid,
            #     displacement=False,
            # )
            # cum_deform_field_bwd = cum_deform_field_bwd[1:, ...] + self.sp_tr(
            #     deform_field_bwd[:Nd - delta_timestep, ...],
            #     bwd_grid,
            #     displacement=False,
            # )
        elif method == "flow_continuation":
            Nd = deform_flow.shape[0]
            cum_deform_field_fwd = cum_deform_field_fwd[:-1, ...] + self._intergate_flow(
                deform_flow[delta_timestep:, ...], dir="fwd", target_grid=fwd_grid)
            cum_deform_field_bwd = cum_deform_field_bwd[1:, ...] + self._intergate_flow(
                deform_flow[:Nd - delta_timestep, ...],
                dir="bwd",
                target_grid=bwd_grid,
            )
        else:
            raise ValueError("field_composition_method must be either 'interpolation' or 'flow_continuity'")
        return cum_deform_field_fwd, cum_deform_field_bwd

    def _gaussian_smooth(self, field, sigma):
        """Smooths the deformation field with a Gaussian filter in the Fourier domain.

        Args:
            field (torch.Tensor): The deformation field to be smoothed.
            sigma (float): The standard deviation of the Gaussian filter.

        Returns:
            torch.Tensor: The smoothed deformation field.
        """
        v_fft = torch.fft.fftn(field, dim=[2, 3, 4])

        freqx = torch.fft.fftfreq(field.shape[2], d=self.deform_pix_dim[0]).to(self.device)
        freqy = torch.fft.fftfreq(field.shape[3], d=self.deform_pix_dim[1]).to(self.device)
        freqz = torch.fft.fftfreq(field.shape[4], d=self.deform_pix_dim[2]).to(self.device)

        freqx, freqy, freqz = torch.meshgrid(freqx, freqy, freqz, indexing="ij")
        omega_f = 1 / (2 * torch.pi * sigma)
        filter_response = torch.exp(-1 / 2 * ((freqx**2 + freqy**2 + freqz**2) / omega_f**2))
        v_fft = filter_response * v_fft

        field_filtered = torch.real(torch.fft.ifftn(v_fft, dim=[2, 3, 4]))
        return field_filtered

    def get_deform_fields(self, deform_flow):
        """Computes the all deformation fields from the deformation flow.
        
        The deformations are organized in a matrix such that the deformation from the deformation that distortes 
        image_i to image_j is given by deform_matrix[j, i]. Another way of viewing the matrix is that the 
        the displacment at time t_j of a particle starting at the identity grid in image_i is given with
        deform_matrix[i, j]. 
        
        Args:
            deform_flow (torch.Tensor or np.ndarray): The deformation flow. Shape ``(N, 3, x, y, z)``
            
        Returns:
            torch.Tensor or np.ndarray: The deformation fields. Shape ``(N, N, 3, x, y, z)``
        """
        # Get the type of the output
        if isinstance(deform_flow, np.ndarray):
            dtype = "np"
        elif isinstance(deform_flow, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("deform_flow must be either numpy array or torch tensor")

        deform_flow = self._convert_to_torch(deform_flow)

        with torch.no_grad():
            deform_matrix = torch.zeros((self.n_img, self.n_img, 3, *self.deform_size), device=self.device)

            deform_field_fwd = self._intergate_flow(deform_flow, "fwd")
            deform_field_bwd = self._intergate_flow(deform_flow, "bwd")

            deform_matrix[utils.get_dig_ind(self.n_img, 1)] = deform_field_fwd
            deform_matrix[utils.get_dig_ind(self.n_img, -1)] = deform_field_bwd

            cum_deform_field_fwd = deform_field_fwd
            cum_deform_field_bwd = deform_field_bwd

            for delta_timestep in range(1, self.n_img - 1):
                (
                    cum_deform_field_fwd,
                    cum_deform_field_bwd,
                ) = self._commute_deform_fields(cum_deform_field_fwd, cum_deform_field_bwd, deform_flow, delta_timestep,
                                                deform_field_fwd, deform_field_bwd)
                deform_matrix[utils.get_dig_ind(self.n_img, delta_timestep + 1)] = cum_deform_field_fwd
                deform_matrix[utils.get_dig_ind(self.n_img, -delta_timestep - 1)] = cum_deform_field_bwd

        if dtype == "np":
            return deform_matrix.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_matrix

    def deform(self, images, deform_field, mode="bilinear", padding_mode="zeros", displacement=True):
        """Transforms the batch of images according to the deformation field.
        
        Args:
            images (torch.Tensor or np.ndarray): The images to deform. Shape ``(N, C, x, y, z)``
            deform_field (torch.Tensor or np.ndarray): The deformation field. Shape ``(N, 3, x, y, z)``
            mode (str, optional): The interpolation mode. Either 'bilinear' or 'nearest'.
            padding_mode (str, optional): The padding mode. Either 'zeros' or 'border'.
            displacement (bool, optional): Whether the deformation field is a displacement field or a deformation field.
            
        Returns:
            torch.Tensor or np.ndarray: The deformed images. Shape ``(N, C, x, y, z)``
        """
        if isinstance(images, np.ndarray):
            dtype = "np"
        elif isinstance(images, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("images must be either numpy array or torch tensor")

        images = self._convert_to_torch(images)
        deform_field = self._convert_to_torch(deform_field)

        if ((mode == self.mode) and np.all(np.array(self.image_size) == np.array(images.shape[2:]))):
            with torch.no_grad():
                def_images = self.sp_tr_img(images, deform_field)
        else:
            sp_tr = SpatialTransformer(
                images.shape[2:],
                self.pix_dim,
                mode=mode,
                padding_mode=padding_mode,
            ).to(self.device)
            def_images = sp_tr(images, deform_field, displacement=displacement)

        if dtype == "np":
            return def_images.detach().cpu().numpy()
        elif dtype == "torch":
            return def_images

    def rigid_transform(self, images, rotations, translations):
        grid = self._get_affine_grid(rotations, translations)
        return self.sp_tr_img(images, grid, displacement=False)

    def linear_transform(self, images, matrix):
        grid = self._get_affine_grid(matrix=matrix)
        return self.sp_tr_img(images, grid, displacement=False)

    def add_linear_transform(self, displacements, matrix):
        if isinstance(displacements, np.ndarray):
            dtype = "np"
        elif isinstance(displacements, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("displacements must be either numpy array or torch tensor")

        displacements = self._convert_to_torch(displacements)
        matrix = self._convert_to_torch(matrix)

        deform_field = displacements + self._get_affine_grid(matrix=matrix)
        # deform_field = torch.cat([deform_field, torch.ones_like(deform_field[:, :1, ...])], dim=1)
        # deform_field = torch.einsum("nji, nihwd -> njhwd", matrix, deform_field)

        deform_field = deform_field[:, :3, ...] - self.sp_tr_img.grid
        if dtype == "np":
            return deform_field.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_field

    def _convert_to_torch(self, data):
        """
        Converts the data to torch tensor.
        """
        if isinstance(data, np.ndarray):
            return torch.from_numpy(data).to(dtype=torch.float32, device=self.device)
        elif isinstance(data, torch.Tensor):
            return data.to(dtype=torch.float32, device=self.device)
        elif isinstance(data, list):
            return torch.tensor(data, dtype=torch.float32, device=self.device)
        else:
            raise TypeError("Data must be either list, numpy array or torch tensor")


class Registration:
    """Multi Stage Temporal Registration.
    Registrates a series of images. The images are first registrated at a low resolution and then the
    resolution is increased and the images are registrated again. At each resolution the deformation field is
    has a resolution relative to the resampled image resolution set by ``deform_res_scale``.
    
    Accepts same arguments as :class:`StageRegistration` but :attr:`iterations` is replaced by 
    :attr:`stages_iterations`, attr:`deform_res_scale` is replaced by :attr:`stages_deform_scales`, and
    :attr:`image_size` is replaced by :attr:`stages_img_scales`.
    
    Example:
        >>> from muster import Registration
        >>> deform_reg = Registration(
        >>>     stages_iterations=[500, 250, 100],
        >>>     stages_img_scales=[4, 2, 1],
        >>>     stages_deform_scales=[4, 2, 2],
        >>>     image_size=[128, 128, 128],
        >>>     pix_dim=[1, 1, 1],
        >>>     device="cuda:0",
        >>> )
        >>> out = deform_reg.fit(images)
    This example will registrate the image first at a resolution of ``[32, 32, 32]`` with a deformation grid of 
    ``[8, 8, 8]``. Then the resolution is increased to ``[64, 64, 64]`` with a deformation grid of ``[32, 32, 32]``.
    Finally the resolution is increased to ``[128, 128, 128]`` with a deformation grid of ``[64, 64, 64]``.
    
    See :class:`StageRegistration` for more information about the arguments.
    
    Args:
        stages_iterations (list of int): Number of iterations for each stage.
        stages_img_scales (list of int): Image rescaling factors for each stage.
        stages_deform_scales (list of int): Deformation rescaling factors for each stage relative to the image rescaling factor.
        """

    def __init__(self, stages_iterations: list, stages_img_scales: list, stages_deform_scales: list, **kwargs):

        # Make sure the arguments are the same length
        if len(stages_iterations) != len(stages_img_scales) or len(stages_img_scales) != len(stages_deform_scales):
            raise ValueError("stages_iterations, stages_img_scales and stages_deform_scales must have the same length")

        self.stages_iterations = stages_iterations
        self.device = kwargs.get("device", "cpu")
        self.image_sizes = []
        self.deform_scales = stages_deform_scales
        self.image_size = kwargs["image_size"]
        self.pix_dim = kwargs["pix_dim"]
        self.deform_sizes = []
        self.pix_dims = []
        self.deform_pix_dims = []
        self.verbose = kwargs.get("verbose", False)
        self.affine_adjustment = kwargs.get("affine_adjustment", "none")

        for stage in range(len(stages_iterations)):
            img_size = np.array(kwargs["image_size"]) // stages_img_scales[stage]
            self.image_sizes.append(img_size.tolist())
            self.pix_dims.append(np.array(kwargs["pix_dim"]) * stages_img_scales[stage])

            self.deform_sizes.append((np.array(self.image_sizes[stage]) // self.deform_scales[stage]).tolist())
            self.deform_pix_dims.append(np.array(self.pix_dims[stage]) * self.deform_scales[stage])

        # Initialize the deformable registration model of the last stage
        self.kwargs = kwargs
        stage_args = self.kwargs
        stage_args["deform_res_scale"] = self.deform_scales[-1]
        stage_args["num_iterations"] = self.stages_iterations[-1]
        stage_args["image_size"] = self.image_sizes[-1]
        stage_args["pix_dim"] = self.pix_dims[-1]

        self.dr = StageRegistration(**stage_args)

    def fit(self, images, timepoints=None, masks=None):
        """
        Fits the registration model to the images
        Args:.
            - images (numpy.ndarray or torch.Tensor): The images to registrate. Shape (N, C, x, y, z)
            - timepoints (numpy.ndarray or torch.Tensor, optional): The timepoints of the images. Shape (N,)
            - masks (numpy.ndarray or torch.Tensor, optional): The masks of the images. Shape (N, C, x, y, z)

        Returns:
            dict: A dictionary containing the deformation field, rotations and translations.
        
        """

        images = self._convert_to_torch(images)

        # Register the images to the current stage resolution
        deform_flow = None
        rotation = None
        translation = None
        affine_matrix = None

        for stage in range(len(self.stages_iterations)):

            # Reinitalize the deformable registration model for the current stage
            stage_args = self.kwargs
            stage_args["deform_res_scale"] = self.deform_scales[stage]
            stage_args["num_iterations"] = self.stages_iterations[stage]
            stage_args["image_size"] = self.image_sizes[stage]
            stage_args["pix_dim"] = self.pix_dims[stage]

            # self.dr = None
            # torch.cuda.empty_cache()
            self.dr = StageRegistration(**stage_args)

            if self.verbose:
                print(
                    f"Stage {stage+1}/{len(self.stages_iterations)} at image resolution {self.image_sizes[stage]}, deformation resolution {self.deform_sizes[stage]}"
                )
            # Resample the images to the current stage resolution
            img_resampled = nn.functional.interpolate(
                images,
                size=self.image_sizes[stage],
                mode="trilinear",
                align_corners=True,
            )
            if masks is not None:
                masks_resampled = nn.functional.interpolate(masks, size=self.image_sizes[stage], mode="nearest")
            else:
                masks_resampled = None

            out = self.dr.fit(
                img_resampled,
                deform_flow,
                timepoints,
                rotation,
                translation,
                affine_matrix,
                masks_resampled,
            )

            deform_flow = torch.tensor(out["deform_flow"], device=self.device)
            if self.affine_adjustment == "rigid":
                rotation = torch.tensor(out["rotations"], device=self.device)
                translation = torch.tensor(out["translations"], device=self.device)
            elif self.affine_adjustment == "affine":
                affine_matrix = torch.tensor(out["affine_matrix"], device=self.device)

            out["deform_flow_not_resampled"] = (deform_flow.detach().cpu().numpy())

            if stage < len(self.stages_iterations) - 1:
                deform_flow = nn.functional.interpolate(
                    deform_flow,
                    size=self.deform_sizes[stage + 1],
                    mode="trilinear",
                    align_corners=True,
                )
        # Add the deform fields
        out["deform_field"] = self.get_deform_fields(deform_flow).detach().cpu().numpy()
        # out["rotations"] = rotation.detach().cpu().numpy()
        # out["translations"] = translation.detach().cpu().numpy()
        #out["affine_matrix"] = affine_matrix.detach().cpu().numpy()
        return out

    def rigid_transform(self, images, rotations, translations):
        """
        Applies the rigid transformation to the images.
        """
        rotations = self._convert_to_torch(rotations)
        translations = self._convert_to_torch(translations)
        return self.dr.rigid_transform(images, rotations, translations)

    def deform(self, images, deform_field, mode="bilinear", padding_mode="zeros", displacement=True):
        """Transforms the batch of images according to the deformation field.
        
        Args:
            images (torch.Tensor or np.ndarray): The images to deform. Shape ``(N, C, x, y, z)``
            deform_field (torch.Tensor or np.ndarray): The deformation field. Shape ``(N, 3, x, y, z)``
            mode (str, optional): The interpolation mode. Either 'bilinear' or 'nearest'.
            padding_mode (str, optional): The padding mode. Either 'zeros' or 'border'.
            displacement (bool, optional): Whether the deformation field is a displacement field or a deformation field.
            
        Returns:
            torch.Tensor or np.ndarray: The deformed images. Shape ``(N, C, x, y, z)``
        """
        return self.dr.deform(images, deform_field, mode, padding_mode, displacement)

    def get_deform_fields(self, deform_flow):
        """Computes the all deformation fields from the deformation flow.
        
        The deformations are organized in a matrix such that the deformation from the deformation that distortes 
        image_i to image_j is given by deform_matrix[j, i]. Another way of viewing the matrix is that the 
        the displacment at time t_j of a particle starting at the identity grid in image_i is given with
        deform_matrix[i, j]. 
        
        Args:
            deform_flow (torch.Tensor or np.ndarray): The deformation flow. Shape ``(N, 3, x, y, z)``
            
        Returns:
            torch.Tensor or np.ndarray: The deformation fields. Shape ``(N, N, 3, x, y, z)``
        """

        if isinstance(deform_flow, np.ndarray):
            dtype = "np"
        elif isinstance(deform_flow, torch.Tensor):
            dtype = "torch"
        else:
            raise TypeError("deform_field must be either numpy array or torch tensor")

        deform_flow = self._convert_to_torch(deform_flow)
        if self.deform_scales[-1] != 1:
            with torch.no_grad():
                deform_matrix = self.dr.get_deform_fields(deform_flow)
                n_img = deform_matrix.shape[0]
                deform_matrix = nn.functional.interpolate(
                    deform_matrix.reshape((-1, 3, *self.deform_sizes[-1])),
                    size=self.image_size,
                    mode="trilinear",
                    align_corners=True,
                )
                deform_matrix = deform_matrix.reshape((
                    n_img,
                    n_img,
                    3,
                    *self.image_size,
                ))
        else:
            with torch.no_grad():
                deform_matrix = self.dr.get_deform_fields(deform_flow)

        if dtype == "np":
            return deform_matrix.detach().cpu().numpy()
        elif dtype == "torch":
            return deform_matrix

    def _convert_to_torch(self, data):
        """Converts the data to torch tensor.
        
        Args:
            data (numpy.ndarray or torch.Tensor): The data to convert.
            
        Returns:
            torch.Tensor: The data converted to torch tensor.
        """
        if isinstance(data, np.ndarray):
            return torch.from_numpy(data).to(dtype=torch.float32, device=self.device)
        elif isinstance(data, torch.Tensor):
            return data.to(dtype=torch.float32, device=self.device)
        else:
            raise TypeError("Data must be either numpy array or torch tensor")

    def get_identity_grid(self):
        return self.dr.sp_tr_img.grid.detach().cpu().numpy()


class SpatialTransformer(nn.Module):
    r""" Spatial Transformer for performing grid pull operations on spaces.
    Based on https://github.com/voxelmorph/voxelmorph. The spatial transformer uses a deformation field to deform
    the input space. The deformation field is given in mm/step. 
    
    Args:
        size (tuple): the target size of the output tensor. shape ``(x, y, z)``
        pix_dim (tuple): the pixel spacing (mm/px) of the output tensor ``(dx, dy, dz)``
        mode ('bilinear' or 'nearest'): interpolation mode
        padding_mode ('zeros' or 'border'): padding mode
    """

    def __init__(self, size, pix_dim=(1, 1, 1), mode="bilinear", padding_mode="zeros"):
        super().__init__()

        self.mode = mode
        self.padding_mode = padding_mode
        self.pix_dim = pix_dim
        self.size = size
        self.size_mm = [(s - 1) * p for s, p in zip(size, pix_dim)]

        # Create sampling grid
        vectors = [torch.linspace(0, self.size_mm[i], steps=self.size[i]) for i in range(len(self.size))]
        grids = torch.meshgrid(vectors, indexing='ij')
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.float)
        self.register_buffer("grid", grid)

    def forward(self, field, transformation_field, displacement=True):
        r"""Deforms the field image with the transformation field with units in mm/step.

            Args:
                field: the source space to be transformed
                transformation_field: the transformation field to be applied to the source image, with units in mm/step
                displacement: whether the transformation field is a displacement field or a deformation field
                
            Returns:
                torch.Tensor: the transformed source image
            """

        if displacement:
            new_locs = self.grid + transformation_field
        else:
            new_locs = transformation_field.clone()

        # need to normalize grid values to [-1, 1] for resampler
        for axis_idx in range(len(self.size_mm)):
            new_locs[:, axis_idx, ...] = 2 * (new_locs[:, axis_idx, ...] / (self.size_mm[axis_idx]) - 0.5)

        if len(self.size_mm) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(self.size_mm) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(
            field,
            new_locs,
            align_corners=True,
            mode=self.mode,
            padding_mode=self.padding_mode,
        )


class AffineRegistration:

    def __init__(self,
                 n_search_angles,
                 stages_img_scales,
                 image_size,
                 pix_dim,
                 mode="bilinear",
                 padding_mode="zeros",
                 device="cpu"):
        self.n_search_angles = n_search_angles
        self.stages_img_scales = stages_img_scales

        self.image_size = (torch.tensor(image_size) + stages_img_scales[0] - 1) // stages_img_scales[0]
        print(f"Image size: {self.image_size}")
        self.pix_dim = np.array(pix_dim) * stages_img_scales[0]
        self.mode = mode
        self.padding_mode = padding_mode
        self.device = device
        self.sp_tr_img = SpatialTransformer(
            size=self.image_size,
            pix_dim=self.pix_dim,
            mode=self.mode,
            padding_mode=self.padding_mode,
        ).to(self.device)

    def fit(self, fixed, movings):
        """ Fits the affine registration model to the images.
        
        Args:
            fixed (torch.Tensor): The fixed image. Shape (1, C, x, y, z)
            movings (torch.Tensor): The moving images. Shape (N, C, x, y, z)
        """

        # Z normalize the images
        fixed /= fixed.std()
        movings /= movings.std(axis=(1, 2, 3, 4), keepdims=True)

        # Downsample the images
        fixed = torch.nn.functional.interpolate(
            fixed, size=tuple(self.image_size), align_corners=True, mode='trilinear')
        movings = torch.nn.functional.interpolate(
            movings, size=tuple(self.image_size), align_corners=True, mode='trilinear')

        with torch.no_grad():
            # Calculate the center of mass of the fixed image
            fixed_cm = self._calculate_cm(fixed)

            # Calculate the center of mass of the moving images
            movings_cm = self._calculate_cm(movings)

            # Initalize the affine matrix
            affine_mats = torch.zeros((movings.shape[0], 3, 4), device=fixed.device)

            # Set the translation
            affine_mats[:, :3, 3] = fixed_cm - movings_cm

            # Calculate the moments of the fixed image
            fixed_moments = self._calculate_moments(fixed, fixed_cm)

            # Calculate the moments of the moving images
            movings_moments = self._calculate_moments(movings, movings_cm).to(fixed.device)

            scale_mat = torch.zeros((movings.shape[0], 3, 3), device=fixed.device)
            # Set the scaling
            for i in range(3):
                affine_mats[:, i, i] = 1
                scale_mat[:, i, i] = movings_moments[:, i] / fixed_moments[:, i]

            rot_mats = self._get_uniform_rotations(self.n_search_angles**3)

            # Repeat the affine matrices for each rotation
            affine_mats = affine_mats.repeat_interleave(self.n_search_angles**3, dim=0)
            movings_cm = movings_cm.repeat_interleave(self.n_search_angles**3, dim=0)

            # Rotate the affine matrices around the center of mass
            affine_mats = self._rotate_affine_mats(affine_mats, rot_mats, movings_cm)
            affine_mats = self._rotate_affine_mats(affine_mats, scale_mat, movings_cm)

        epochs = 200
        fixed = fixed.repeat_interleave(self.n_search_angles**3 * movings.shape[0], dim=0).detach().clone()
        movings = movings.repeat_interleave(self.n_search_angles**3, dim=0).detach().clone()
        print(affine_mats)
        affine_mats = torch.nn.Parameter(affine_mats, requires_grad=True)
        #optimizer = torch.optim.Adam([affine_mats], lr=0.001)
        #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)

        options = {'lr': 0.01, 'gtol': 1e-24, 'xtol': 1e-24}
        #optimizer = Minimizer([affine_mats], method='bfgs', max_iter=epochs, disp=2, options=options)

        # #loss_fn = losses.GaussNCC(13, image_size=self.image_size, pix_dim=self.pix_dim, reduce=False)
        loss_fn = losses.MutualInformation(sigma=0.01, num_bins=32, normalize=True, reduce=False).to(self.device)

        #loss_fn = losses.WNCC(3, reduce=False)

        #loss_fn = losses.NCC(3, reduce=False)

        # for epoch in range(epochs):
        #     optimizer.zero_grad()

        #     warped_movings = self._affine_transform(movings, affine_mats, cm=movings_cm)

        #     #loss = torch.mean((fixed - warped_movings)**2, dim=(1, 2, 3, 4))

        #     loss = loss_fn.loss(fixed, warped_movings)
        #     best_loss, best_idx = torch.min(loss, dim=0)

        #     loss_mean = loss.mean()
        #     loss_mean.backward()
        #     optimizer.step()
        #     scheduler.step()

        #     print(f"Epoch {epoch+1}/{epochs}, loss: {best_loss.item()}, idx: {best_idx.item()}")

        def closure():
            optimizer.zero_grad()
            warped_movings = self._affine_transform(movings, affine_mats, cm=movings_cm)
            loss = loss_fn.loss(fixed, warped_movings)
            #loss = torch.mean((fixed - warped_movings)**2, dim=(1, 2, 3, 4))
            loss_mean = loss.mean()
            # Calculate the determinant of the affine matrix
            det = torch.det(affine_mats[:, :3, :3])
            #loss_mean += 0.02 * torch.mean((det - 1)**2)

            return loss_mean

        loss = optimizer.step(closure)

        # Compute one last time to get the best affine matrix
        warped_movings = self._affine_transform(movings, affine_mats, cm=movings_cm)
        loss = loss_fn.loss(fixed, warped_movings)

        # Return the best affine matrix
        best_loss, best_idx = torch.min(loss, dim=0)
        print(f"Best loss: {best_loss.item()}, idx: {best_idx.item()}")
        return affine_mats[best_idx][None, ...]

        # top_val, top_ind = torch.topk(loss, 4, largest=False, dim=0)
        # return affine_mats[top_ind]

    def _calculate_cm(self, images):
        """Calculates the center of mass of the images.
        
        Args:
            images (torch.Tensor): The images. Shape (N, C, x, y, z)
            
        Returns:
            torch.Tensor: The center of mass of the images. Shape (N, 3)
        """
        # Create the grid
        grid = torch.stack(
            torch.meshgrid(
                torch.arange(images.shape[2], device=images.device) * self.pix_dim[0],
                torch.arange(images.shape[3], device=images.device) * self.pix_dim[1],
                torch.arange(images.shape[4], device=images.device) * self.pix_dim[2],
                indexing="ij",
            ))
        grid = grid.type(torch.float32)[None, ...]
        # Calculate the center of mass
        cm = torch.sum(images * grid, dim=(2, 3, 4)) / torch.sum(images, dim=(2, 3, 4))
        return cm

    def _calculate_moments(self, images, cm):
        """Calculates the moments of the images.
        
        Args:
            images (torch.Tensor): The images. Shape (N, C, x, y, z)
            cm (torch.Tensor): The center of mass of the images. Shape (N, 3)
            
        Returns:
            torch.Tensor: The moments of the images. Shape (N, 3)
        """
        # Create the grid
        grid = torch.stack(
            torch.meshgrid(
                torch.arange(images.shape[2], device=images.device) * self.pix_dim[0],
                torch.arange(images.shape[3], device=images.device) * self.pix_dim[1],
                torch.arange(images.shape[4], device=images.device) * self.pix_dim[2],
                indexing="ij",
            ))
        grid = grid.type(torch.float32)[None, ...]
        # Calculate the moments
        moments = torch.sqrt(
            torch.sum(images * (grid - cm[:, :, None, None, None])**2, dim=(2, 3, 4)) /
            torch.sum(images**2, dim=(2, 3, 4)))

        print(moments)

        return moments

    def _get_uniform_rotations(self, n_angles):
        """
        Taken from: https://www.blopig.com/blog/2021/08/uniformly-sampled-3d-rotation-matrices/
        Apply a random rotation matrix in 3D, with a distribution uniform over the
        sphere.
        Algorithm taken from "Fast Random Rotation Matrices" (James Avro, 1992):
        https://doi.org/10.1016/B978-0-08-050755-2.50034-8
        Arg:
            x: vector or set of vectors with dimension (n, 3), where n is the
                number of vectors
        Returns:
            Array of shape (n, 3) containing the randomly rotated vectors of x,
            about the mean coordinate of x.
        """
        # There are two random variables in [0, 1) here (naming is same as paper)

        x1 = torch.rand(n_angles, device=self.device)
        x2 = torch.rand(n_angles, device=self.device)
        x3 = torch.rand(n_angles, device=self.device)

        x2 = 2 * np.pi * x2

        print(f"x1: {x1}")
        print(f"x2: {x2}")
        print(f"x3: {x3}")

        # Generate random rotation matrix about the z axis.
        R = torch.eye(3).repeat(x1.shape[0], 1, 1).to(x1.device)

        R[:, 0, 0] = R[:, 1, 1] = torch.cos(2 * torch.pi * x1)
        R[:, 0, 1] = -torch.sin(2 * torch.pi * x1)
        R[:, 1, 0] = -R[:, 0, 1]

        # Rotation of all points around x axis using matrix
        v = torch.stack([torch.cos(x2) * torch.sqrt(x3), torch.sin(x2) * torch.sqrt(x3), torch.sqrt(1 - x3)], dim=1)
        H = torch.eye(3, device=self.device).repeat(x1.shape[0], 1, 1) - (2 * torch.einsum("ni, nj -> nij", v, v))
        M = -torch.einsum("nij, njk -> nik", H, R)
        return M

    def _rotate_affine_mats(self, affine_mats, rot_mats, cm):
        """Rotates the affine matrices around the center of mass.

        Args:
            affine_mats (torch.Tensor): The affine matrices. Shape (N, 3, 4)
            rot_mats (torch.Tensor): The rotation matrices. Shape (N, 3, 3)
            cm (torch.Tensor): The center of mass of the images. Shape (N, 3)

        Returns:
            torch.Tensor: The rotated affine matrices. Shape (N, 3, 4)
        """

        # Move the center of mass to the origin
        #affine_mats[:, :3, 3] = affine_mats[:, :3, 3] - cm
        # Rotate the affine matrices
        affine_mats[:, :3, :3] = torch.einsum("nij, njk -> nik", rot_mats, affine_mats[:, :3, :3])
        affine_mats[:, :3, 3] = torch.einsum("nij, nj -> ni", rot_mats, affine_mats[:, :3, 3])
        # Move the center of mass back
        #affine_mats[:, :3, 3] = affine_mats[:, :3, 3] + cm
        return affine_mats

    def combine_affine_matrices(self, mat1, mat2):
        """
        Assumes the matrices are in the shape (N, 3, 4)
        """
        mat1 = torch.concat([mat1, torch.zeros_like(mat1[:, :1, :])], dim=1)
        mat1[:, 3, 3] = 1

        mat2 = torch.concat([mat2, torch.zeros_like(mat2[:, :1, :])], dim=1)
        mat2[:, 3, 3] = 1

        mat = torch.einsum("nij, njk -> nik", mat1, mat2)
        return mat[:, :3, :4]

    def _get_affine_grid(self, rotation=None, translation=None, matrix=None, type='img', cm=None):
        """
        Computes the affine grid from rotation and translation
        """

        if matrix is None:
            matrix = self._get_homogeneous_transformation_matrix(rotation, translation)

        if type == 'img':
            grid = self.sp_tr_img.grid.detach().clone().repeat(matrix.shape[0], 1, 1, 1, 1)
        else:
            grid = self.sp_tr.grid.detach().clone().repeat(matrix.shape[0], 1, 1, 1, 1)

        # Grid is in the shape (N, 3, H, W, D)
        # Matrix is in the shape (N, 3, 4)
        # Move grid to center of image
        #print(f"Max of grid: {torch.max(grid, dim=1)}")
        #print(f"Estiamted grid max: {self.image_size * self.pix_dim}")
        # grid[:, :3, :] = grid[:, :3, :] - torch.tensor(
        #     self.image_size * self.pix_dim, device=self.device)[None, :, None, None, None] / 2
        # Rotate around the center of mass

        # Add [0, 0, 0, 1]

        cm = torch.tensor(self.sp_tr_img.size_mm) / 2

        cm_matrix = torch.zeros_like(matrix)
        cm_matrix[:, :3, :3] = torch.eye(3, device=self.device)
        cm_matrix[:, :3, 3] = cm

        matrix = self.combine_affine_matrices(cm_matrix, matrix)
        cm_matrix[:, :3, 3] = -cm
        matrix = self.combine_affine_matrices(matrix, cm_matrix)

        return torch.einsum("nij, njhwd -> nihwd", matrix[:, :3, :3], grid) + matrix[:, :3, 3, None, None, None]

    def _affine_transform(self, images, matrix, cm=None):
        return self.sp_tr_img(images, self._get_affine_grid(matrix=matrix, cm=cm), displacement=False)
